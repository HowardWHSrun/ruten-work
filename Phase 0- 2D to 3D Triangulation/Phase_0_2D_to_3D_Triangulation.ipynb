{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffe254-6a1c-4359-8e30-4b03dec25ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5224c-9971-42ab-9616-7983c745353d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346c74e-753a-415b-989d-45a40c063e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calib] cameras: ['cam-topleft.mp4', 'cam-topright.mp4', 'cam-bottomleft.mp4', 'cam-bottomright.mp4'] img_size: (600, 600)\n",
      "[warn] no analysis.h5 for cam-topleft.mp4 (pattern *cam-topleft*.analysis.h5)\n",
      "[warn] no analysis.h5 for cam-topright.mp4 (pattern *cam-topright*.analysis.h5)\n",
      "[cam-bottomleft.mp4] loaded (13583, 9, 2) frames, fps≈unknown\n",
      "[cam-bottomright.mp4] loaded (13335, 9, 2) frames, fps≈unknown\n",
      "Using FPS = 120.0\n",
      "[align] applied lags: {'cam-bottomleft.mp4': 0, 'cam-bottomright.mp4': 106} → new common T = 13335\n",
      "[clean] Per-cam lengths: {'cam-bottomleft.mp4': 13335, 'cam-bottomright.mp4': 13335} → using T = 13335\n",
      "[match] cam-bottomright.mp4 → cam-bottomleft.mp4: median assigned costs (first 10) = [69.5 62.1  8.4  8.9 55.4 21.4 31.  45.  46.3]\n",
      "[match] aligned node ordering across cameras using geometric consistency.\n",
      "[swap] U/L aliases not found → skipping 2D swap check.\n",
      "[tri+fill] per-node coverage (first 10): [0.    0.    0.92  0.804 0.    0.073 0.02  0.002 0.002]\n",
      "[OK] wrote all nodes 3D arrays: C:\\Users\\Lenovo\\Desktop\\Howard Work\\out_sleap\\all_nodes_3d.npz\n",
      "[gape] using nodes: U=node_4  L=node_3  (indices 3,2)\n",
      "[OK] wrote UL 3D tracks (csv/npz).\n",
      "[debug] gape_s_mm percentiles: min -0.1, p25 1.6, med 2.2, p75 2.9, p95 5.6\n",
      "[DONE] Wrote gape time-series and plots.\n"
     ]
    }
   ],
   "source": [
    "# === 2D → 3D + Gape (Refined for Phase 1 style) ===\n",
    "# This cell aligns to the structure used in other Phase 1 notebooks.\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Config\n",
    "# --------------------------------------------------\n",
    "from pathlib import Path\n",
    "import os, json, glob, h5py, re, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from numpy.random import default_rng\n",
    "import cv2\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import datetime\n",
    "\n",
    "# Base paths\n",
    "BASE_DIR = Path(\"/Users/howardwang/Desktop/Ruten/Evaluation-Metrics_Vishal-main\")\n",
    "SESSION = \"2025-05-28_14-12-04_124591\"\n",
    "INFER_ROOT = BASE_DIR / \"face/results/n20/inference_raw\"\n",
    "CALIB_JSON = BASE_DIR / \"Calibration/calibration.json\"\n",
    "OUTPUT_TAG = \"v1\"  # increment if re-running\n",
    "\n",
    "# Cameras to use\n",
    "CAM_ORDER = [\n",
    "    \"cam-topright\",\n",
    "    \"cam-topleft\",\n",
    "    \"cam-bottomright\",\n",
    "    \"cam-bottomleft\",\n",
    "]\n",
    "\n",
    "# Input file mapping (predictions.h5 per camera)\n",
    "PRED_FILES = {\n",
    "    cam: INFER_ROOT / cam / \"predictions.h5\" for cam in CAM_ORDER\n",
    "}\n",
    "\n",
    "# Output/session directories (non-destructive)\n",
    "OUTDIR = BASE_DIR / \"face/results/n20/triangulated\" / f\"session_{SESSION}_{OUTPUT_TAG}\"\n",
    "WORKDIR = OUTDIR / \"work\"\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_PATH = OUTDIR / \"pipeline_log.txt\"\n",
    "DEBUG_LOG_PATH = BASE_DIR / \".cursor\" / \"debug.log\"\n",
    "\n",
    "# Filters / params\n",
    "FPS_FALLBACK = 120.0\n",
    "MAX_SPEED_PX = 40          # 2D speed cap (px/frame) for outlier removal\n",
    "MAX_GAP_2D = 8             # frames\n",
    "MAX_GAP_3D_SEC = 0.30\n",
    "REPROJ_STRICT = 10.0       # px\n",
    "REPROJ_LOOSE = 18.0        # px (only if previous frame was good)\n",
    "LAG_WINDOW_SEC = 0.5       # fallback lag search +/- window\n",
    "SAMPLE_FRAMES_ALIGN = 800  # frames for node matching/lag estimation\n",
    "SAMPLE_NODES_ALIGN = 6\n",
    "\n",
    "# Suffix for outputs\n",
    "SUFFIX = OUTPUT_TAG\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Utility: logger\n",
    "# --------------------------------------------------\n",
    "def log(msg):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    print(line)\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "def debug_log(hypothesis_id, location, message, data):\n",
    "    \"\"\"Write debug log entry in NDJSON format.\"\"\"\n",
    "    import json\n",
    "    entry = {\n",
    "        \"sessionId\": \"debug-session\",\n",
    "        \"runId\": \"run1\",\n",
    "        \"hypothesisId\": hypothesis_id,\n",
    "        \"location\": location,\n",
    "        \"message\": message,\n",
    "        \"data\": data,\n",
    "        \"timestamp\": int(datetime.datetime.now().timestamp() * 1000)\n",
    "    }\n",
    "    try:\n",
    "        with open(DEBUG_LOG_PATH, \"a\") as f:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "log(\"Starting 2D→3D pipeline\")\n",
    "log(f\"Session: {SESSION}\")\n",
    "log(f\"Outputs: {OUTDIR}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Aliases for nodes\n",
    "# --------------------------------------------------\n",
    "ALIAS = {\n",
    "    \"U\": [\"u\", \"upper\", \"upper_lip\", \"upperlip\", \"u_star\", \"upperlipmarker\",\n",
    "          \"toplip\", \"top_lip\", \"lip_u\", \"ulip\"],\n",
    "    \"L\": [\"l\", \"lower\", \"lower_lip\", \"lowerlip\", \"l_star\", \"lowerlipmarker\",\n",
    "          \"bottomlip\", \"bot_lip\", \"lip_l\", \"llip\"],\n",
    "    \"HEAD\": [\"head\", \"snout\", \"nose\", \"forehead\", \"ha\", \"hb\", \"hc\"],\n",
    "}\n",
    "MOUTHY = [\"lip\", \"mouth\", \"jaw\", \"mandible\", \"maxilla\", \"muzzle\"]\n",
    "\n",
    "# camera-key ↔ calibration key mapping\n",
    "# Calibration uses keys like \"cam-topleft.mp4\", but we use \"cam-topleft\" in CAM_ORDER\n",
    "CAM_KEY = {\n",
    "    \"cam-topleft\": \"cam-topleft.mp4\",\n",
    "    \"cam-topright\": \"cam-topright.mp4\",\n",
    "    \"cam-bottomleft\": \"cam-bottomleft.mp4\",\n",
    "    \"cam-bottomright\": \"cam-bottomright.mp4\",\n",
    "}\n",
    "\n",
    "# ------------ FLEXIBLE ALIASES -------------\n",
    "ALIAS = {\n",
    "    \"U\": [\"u\", \"upper\", \"upper_lip\", \"upperlip\", \"u_star\", \"upperlipmarker\",\n",
    "          \"toplip\", \"top_lip\", \"lip_u\", \"ulip\"],\n",
    "    \"L\": [\"l\", \"lower\", \"lower_lip\", \"lowerlip\", \"l_star\", \"lowerlipmarker\",\n",
    "          \"bottomlip\", \"bot_lip\", \"lip_l\", \"llip\"],\n",
    "    \"HEAD\": [\"head\", \"snout\", \"nose\", \"forehead\", \"ha\", \"hb\", \"hc\"]\n",
    "}\n",
    "# extra name hints for mouth region (used by auto selector)\n",
    "MOUTHY = [\"lip\", \"mouth\", \"jaw\", \"mandible\", \"maxilla\", \"muzzle\"]\n",
    "\n",
    "# ------------ LOAD CALIBRATION -------------\n",
    "with open(CALIB_JSON, \"r\") as cf:\n",
    "    calib = json.load(cf)\n",
    "\n",
    "# Calibration uses keys like \"cam-topleft.mp4\"\n",
    "Ps_raw    = {k: np.array(v) for k, v in calib[\"P\"].items()}\n",
    "Ks_raw    = {k: np.array(calib[\"intrinsics\"][k][\"K\"]) for k in Ps_raw}\n",
    "Dists_raw = {k: np.array(calib[\"intrinsics\"][k][\"dist\"]) for k in Ps_raw}\n",
    "IMG_SIZE = tuple(calib.get(\"img_size\", [None, None]))\n",
    "log(f\"[calib] cameras in file: {list(Ps_raw.keys())}, img_size: {IMG_SIZE}\")\n",
    "\n",
    "# Map camera names (from CAM_ORDER) to calibration keys\n",
    "Ps = {}\n",
    "Ks = {}\n",
    "Dists = {}\n",
    "for cam in CAM_ORDER:\n",
    "    calib_key = CAM_KEY.get(cam)\n",
    "    if calib_key and calib_key in Ps_raw:\n",
    "        Ps[cam] = Ps_raw[calib_key]\n",
    "        Ks[cam] = Ks_raw[calib_key]\n",
    "        Dists[cam] = Dists_raw[calib_key]\n",
    "        # #region agent log\n",
    "        debug_log(\"H3\", f\"calib_mapping_{cam}\", \"Calibration mapping\", {\n",
    "            \"cam\": cam, \"calib_key\": calib_key, \"P_shape\": list(Ps[cam].shape),\n",
    "            \"K_shape\": list(Ks[cam].shape), \"dist_shape\": list(Dists[cam].shape)\n",
    "        })\n",
    "        # #endregion\n",
    "    else:\n",
    "        log(f\"[warn] calibration missing for {cam} (calib key: {calib_key})\")\n",
    "        # #region agent log\n",
    "        debug_log(\"H3\", f\"calib_missing_{cam}\", \"Calibration missing\", {\n",
    "            \"cam\": cam, \"calib_key\": calib_key, \"available_keys\": list(Ps_raw.keys())\n",
    "        })\n",
    "        # #endregion\n",
    "\n",
    "if len(Ps) < 2:\n",
    "    raise RuntimeError(f\"Need ≥2 cameras with calibration. Found: {list(Ps.keys())}\")\n",
    "\n",
    "# ------------ HELPERS -------------\n",
    "def interp_short_nan_runs(y, max_gap):\n",
    "    \"\"\"Linear-fill NaN runs up to max_gap samples.\"\"\"\n",
    "    y = y.copy(); n = len(y); idx = np.arange(n)\n",
    "    good = np.isfinite(y)\n",
    "    if good.sum() < 2: \n",
    "        return y\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if good[i]:\n",
    "            i += 1; \n",
    "            continue\n",
    "        j = i\n",
    "        while j < n and not good[j]:\n",
    "            j += 1\n",
    "        gap = j - i\n",
    "        if 0 < gap <= max_gap and i > 0 and j < n and good[i-1] and good[j]:\n",
    "            y[i:j] = np.interp(idx[i:j], [i-1, j], [y[i-1], y[j]])\n",
    "            good[i:j] = True\n",
    "        i = j\n",
    "    return y\n",
    "\n",
    "def load_sleap_analysis_h5(path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        pts [T, N, 2] in pixels (NaNs where missing),\n",
    "        node_names [N],\n",
    "        fps (float or None),\n",
    "        frame_idx [T] (int) or None\n",
    "\n",
    "    Supports:\n",
    "      1) Classic SLEAP arrays: (K,T,N,2) / (T,N,2) / variants\n",
    "      2) \"Flat predictions\" layout:\n",
    "         /predictions/{i}.x, /predictions/{i}.y, optional /predictions/{i}.score\n",
    "         /predictions/frame_idx, /predictions/track\n",
    "    \"\"\"\n",
    "    import h5py, numpy as np, re\n",
    "\n",
    "    def _extract_fps(f):\n",
    "        fps = None\n",
    "        for k in [\"video_fps\", \"videos/fps\", \"video/fps\"]:\n",
    "            if k in f:\n",
    "                v = f[k][()]\n",
    "                fps = float(v[0] if np.size(v) else v); break\n",
    "        return fps\n",
    "\n",
    "    def _extract_names(f, N):\n",
    "        for k in [\"node_names\",\"nodes\",\"points/labels\",\"tracks/labels\",\"labels\"]:\n",
    "            if k in f:\n",
    "                try:\n",
    "                    nn = [x.decode(\"utf-8\") if isinstance(x, bytes) else str(x)\n",
    "                          for x in np.array(f[k]).ravel()]\n",
    "                    return nn\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return [f\"node_{i+1}\" for i in range(N)]\n",
    "\n",
    "    def _try_classic(f):\n",
    "        best = None\n",
    "        def walk(g, prefix=\"\"):\n",
    "            nonlocal best\n",
    "            for k, v in g.items():\n",
    "                p = f\"{prefix}/{k}\" if prefix else f\"/{k}\"\n",
    "                if isinstance(v, h5py.Group):\n",
    "                    walk(v, p)\n",
    "                else:\n",
    "                    if v.ndim in (3,4) and v.shape[-1] == 2 and np.issubdtype(v.dtype, np.number):\n",
    "                        s = 0\n",
    "                        if v.ndim == 4: s += 3\n",
    "                        if v.shape[-1] == 2: s += 2\n",
    "                        if max(v.shape) > 50: s += 1\n",
    "                        if best is None or s > best[0]:\n",
    "                            best = (s, p, tuple(v.shape))\n",
    "        walk(f)\n",
    "        if not best:\n",
    "            return None\n",
    "        ds = np.array(f[best[1]])\n",
    "        # normalize to (K,T,N,2)\n",
    "        if ds.ndim == 4:\n",
    "            s = ds.shape\n",
    "            if s[1] == 2 and s[-1] > 50:          ds = ds.transpose(0,3,2,1)\n",
    "            elif s[2] == 2 and s[0] > 50:         ds = ds.transpose(3,0,1,2)\n",
    "            elif s[0] == 2:                        ds = ds.transpose(3,2,1,0)\n",
    "            elif s[-1] != 2:\n",
    "                dims = list(s); coord_ax = int(np.where(np.array(dims)==2)[0][0])\n",
    "                frame_ax = int(np.argmax(dims))\n",
    "                candidates = [i for i in range(4) if i != coord_ax]\n",
    "                k_ax = candidates[int(np.argmin([dims[i] for i in candidates]))]\n",
    "                n_ax = [i for i in range(4) if i not in (coord_ax, frame_ax, k_ax)][0]\n",
    "                ds = ds.transpose(k_ax, frame_ax, n_ax, coord_ax)\n",
    "        elif ds.ndim == 3 and ds.shape[-1] == 2:\n",
    "            ds = ds[None, ...]\n",
    "        else:\n",
    "            return None\n",
    "        K, T, N, _ = ds.shape\n",
    "        kbest = 0 if K == 1 else int(np.argmax(np.sum(np.isfinite(ds).all(axis=-1), axis=(1,2))))\n",
    "        pts = ds[kbest].astype(float)\n",
    "        names = _extract_names(f, N)\n",
    "        fps = _extract_fps(f)\n",
    "        # optional scores\n",
    "        def find_score_key():\n",
    "            keys = [\"point_scores\",\"points_scores\",\"node_scores\",\"point_confidences\",\"scores\"]\n",
    "            for k in keys:\n",
    "                if k in f: return k\n",
    "            for grp in f.keys():\n",
    "                if isinstance(f[grp], h5py.Group):\n",
    "                    for k in keys:\n",
    "                        if k in f[grp]: return f\"{grp}/{k}\"\n",
    "            return None\n",
    "        sk = find_score_key()\n",
    "        if sk is not None:\n",
    "            sc = np.array(f[sk]).squeeze()\n",
    "            TT, NN = pts.shape[0], pts.shape[1]\n",
    "            if sc.ndim == 3 and sc.shape[-1] in (1,2): sc = sc[...,0]\n",
    "            elif sc.ndim == 2 and sc.shape == (NN,TT): sc = sc.T\n",
    "            elif sc.ndim == 1 and sc.shape[0] == NN:   sc = np.tile(sc[None,:], (TT,1))\n",
    "            elif sc.ndim == 1 and sc.shape[0] == TT:   sc = np.tile(sc[:,None], (1,NN))\n",
    "            elif sc.ndim != 2:                         sc = None\n",
    "            if sc is not None:\n",
    "                bad = ~np.isfinite(sc) | (sc <= 0)\n",
    "                pts[bad] = np.nan\n",
    "        # no frame_idx here\n",
    "        return pts, names, fps, None\n",
    "\n",
    "    def _try_flat_predictions(f):\n",
    "        if \"predictions\" not in f: return None\n",
    "        g = f[\"predictions\"]\n",
    "        pat = re.compile(r\"^(\\d+)\\.(x|y|score)$\")\n",
    "        nodes = set()\n",
    "        for k in g.keys():\n",
    "            m = pat.match(k)\n",
    "            if m and m.group(2) in (\"x\",\"y\"):\n",
    "                nodes.add(int(m.group(1)))\n",
    "        if not nodes:\n",
    "            return None\n",
    "        idxs = sorted(nodes)\n",
    "        T = g[f\"{idxs[0]}.x\"].shape[0]\n",
    "        N = len(idxs)\n",
    "        pts = np.full((T, N, 2), np.nan, dtype=float)\n",
    "        for j, nid in enumerate(idxs):\n",
    "            x = np.array(g[f\"{nid}.x\"], dtype=float)\n",
    "            y = np.array(g[f\"{nid}.y\"], dtype=float)\n",
    "            pts[:, j, 0] = x\n",
    "            pts[:, j, 1] = y\n",
    "            sk = f\"{nid}.score\"\n",
    "            if sk in g:\n",
    "                sc = np.array(g[sk], dtype=float)\n",
    "                bad = ~np.isfinite(sc) | (sc <= 0)\n",
    "                pts[bad, j, :] = np.nan\n",
    "        names = _extract_names(f, N)\n",
    "        fps = _extract_fps(f)\n",
    "        frame_idx = None\n",
    "        if \"frame_idx\" in g:\n",
    "            frame_idx = np.array(g[\"frame_idx\"]).astype(int)\n",
    "        return pts, names, fps, frame_idx\n",
    "\n",
    "    def _try_sleap_v1_format(f):\n",
    "        \"\"\"Handle SLEAP v1 format with frames, instances, pred_points.\"\"\"\n",
    "        if \"pred_points\" not in f or \"frames\" not in f or \"instances\" not in f:\n",
    "            return None\n",
    "        \n",
    "        frames = np.array(f[\"frames\"])\n",
    "        instances = np.array(f[\"instances\"])\n",
    "        pred_points = np.array(f[\"pred_points\"])\n",
    "        \n",
    "        if len(pred_points) == 0 or len(instances) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Determine number of nodes from first instance\n",
    "        first_inst = instances[0]\n",
    "        pid_start = int(first_inst[\"point_id_start\"])\n",
    "        pid_end = int(first_inst[\"point_id_end\"])\n",
    "        N = pid_end - pid_start\n",
    "        \n",
    "        # Get frame indices\n",
    "        frame_idx = frames[\"frame_idx\"] if \"frame_idx\" in frames.dtype.names else None\n",
    "        \n",
    "        # Build frame -> instances mapping (sorted by score, take best)\n",
    "        frame_to_instances = {}\n",
    "        for i, inst in enumerate(instances):\n",
    "            fid = int(inst[\"frame_id\"])\n",
    "            if fid not in frame_to_instances:\n",
    "                frame_to_instances[fid] = []\n",
    "            frame_to_instances[fid].append((i, float(inst[\"score\"])))\n",
    "        \n",
    "        # Sort instances by score (descending) for each frame\n",
    "        for fid in frame_to_instances:\n",
    "            frame_to_instances[fid].sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get unique frame IDs and sort\n",
    "        unique_frame_ids = sorted(frame_to_instances.keys())\n",
    "        T = len(unique_frame_ids)\n",
    "        \n",
    "        # Initialize output array\n",
    "        pts = np.full((T, N, 2), np.nan, dtype=float)\n",
    "        \n",
    "        # Fill in points for each frame (use best instance)\n",
    "        for t, frame_id in enumerate(unique_frame_ids):\n",
    "            if not frame_to_instances[frame_id]:\n",
    "                continue\n",
    "            # Use the best (first) instance for this frame\n",
    "            inst_idx, _ = frame_to_instances[frame_id][0]\n",
    "            inst = instances[inst_idx]\n",
    "            pid_start = int(inst[\"point_id_start\"])\n",
    "            pid_end = int(inst[\"point_id_end\"])\n",
    "            \n",
    "            if pid_start < len(pred_points) and pid_end <= len(pred_points):\n",
    "                for node_idx in range(N):\n",
    "                    pid = pid_start + node_idx\n",
    "                    if pid < len(pred_points):\n",
    "                        pt = pred_points[pid]\n",
    "                        # Check visible flag (complete may be False even for valid points in SLEAP v1)\n",
    "                        if pt[\"visible\"]:\n",
    "                            score = float(pt[\"score\"]) if \"score\" in pt.dtype.names else 1.0\n",
    "                            if score > 0:\n",
    "                                pts[t, node_idx, 0] = float(pt[\"x\"])\n",
    "                                pts[t, node_idx, 1] = float(pt[\"y\"])\n",
    "        \n",
    "        # Extract node names from metadata if available\n",
    "        names = _extract_names(f, N)\n",
    "        fps = _extract_fps(f)\n",
    "        \n",
    "        return pts, names, fps, frame_idx\n",
    "\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        got = _try_classic(f)\n",
    "        if got is not None:\n",
    "            return got\n",
    "        got = _try_flat_predictions(f)\n",
    "        if got is not None:\n",
    "            return got\n",
    "        got = _try_sleap_v1_format(f)\n",
    "        if got is not None:\n",
    "            return got\n",
    "        raise RuntimeError(f\"No usable points dataset found in {path}.\")\n",
    "\n",
    "\n",
    "\n",
    "def find_alias_indices(node_names, alias_dict):\n",
    "    names_lc = [n.lower() for n in node_names]\n",
    "    def find_one(cands):\n",
    "        for i, n in enumerate(names_lc):\n",
    "            for c in cands:\n",
    "                if c in n:\n",
    "                    return i\n",
    "        return None\n",
    "    idxU = find_one(alias_dict[\"U\"])\n",
    "    idxL = find_one(alias_dict[\"L\"])\n",
    "    idxH = [i for i, n in enumerate(names_lc) for c in alias_dict[\"HEAD\"]\n",
    "            if c in n and i not in (idxU, idxL)][:3]\n",
    "    return idxU, idxL, idxH\n",
    "\n",
    "def name_matches_any(name, substrs):\n",
    "    n = name.lower()\n",
    "    return any(s in n for s in substrs)\n",
    "\n",
    "# ------------ LOAD ALL CAMS -------------\n",
    "per_cam_pts = {}\n",
    "per_cam_frameidx = {}\n",
    "node_names_ref, fps_guess = None, None\n",
    "\n",
    "for cam in CAM_ORDER:\n",
    "    pred_path = PRED_FILES[cam]\n",
    "    if not pred_path.exists():\n",
    "        log(f\"[warn] missing predictions for {cam}: {pred_path}\")\n",
    "        continue\n",
    "    pts, node_names, fps, fidx = load_sleap_analysis_h5(str(pred_path))\n",
    "    if node_names_ref is None:\n",
    "        node_names_ref = node_names\n",
    "    else:\n",
    "        if node_names != node_names_ref:\n",
    "            log(f\"[warn] node order differs for {cam}; will realign later\")\n",
    "    per_cam_pts[cam] = pts\n",
    "    per_cam_frameidx[cam] = fidx  # may be None\n",
    "    if fps:\n",
    "        fps_guess = fps\n",
    "    log(f\"[{cam}] loaded {pts.shape} frames, fps≈{fps or 'unknown'} from {pred_path}\")\n",
    "    # #region agent log\n",
    "    valid_pts = np.isfinite(pts).sum()\n",
    "    total_pts = pts.size\n",
    "    debug_log(\"H1\", f\"load_after_{cam}\", \"2D points validity after loading\", {\n",
    "        \"cam\": cam, \"shape\": list(pts.shape), \"valid_count\": int(valid_pts), \n",
    "        \"total_count\": int(total_pts), \"valid_fraction\": float(valid_pts / total_pts) if total_pts > 0 else 0.0\n",
    "    })\n",
    "    # #endregion\n",
    "\n",
    "if len(per_cam_pts) < 2:\n",
    "    raise RuntimeError(\"Need ≥2 cameras with tracks to triangulate.\")\n",
    "\n",
    "# verify calibration keys exist (map cam names to calibration keys)\n",
    "for cam in per_cam_pts.keys():\n",
    "    calib_key = CAM_KEY.get(cam)\n",
    "    if calib_key is None or calib_key not in Ps_raw:\n",
    "        raise KeyError(f\"Calibration missing for camera '{cam}' (calib key: '{calib_key}'). Check CALIB_JSON keys vs CAM_KEY mapping.\")\n",
    "    if cam not in Ps or cam not in Ks or cam not in Dists:\n",
    "        raise KeyError(f\"Calibration mapping failed for camera '{cam}'. Check CAM_KEY mapping.\")\n",
    "\n",
    "FPS = float(fps_guess or 120.0)\n",
    "print(\"Using FPS =\", FPS)\n",
    "\n",
    "# ------------ ALIGN CAMERAS IN TIME -------------\n",
    "cams = list(per_cam_pts.keys())\n",
    "ref_cam = cams[0]\n",
    "\n",
    "def shift_with_nans(arr, shift):\n",
    "    \"\"\"Positive shift moves data forward (pads front); negative shift pads end.\"\"\"\n",
    "    T = arr.shape[0]\n",
    "    out = np.full_like(arr, np.nan)\n",
    "    if shift == 0:\n",
    "        return arr.copy()\n",
    "    if shift > 0:\n",
    "        out[shift:] = arr[:T-shift]\n",
    "    else:\n",
    "        s = -shift\n",
    "        out[:T-s] = arr[s:]\n",
    "    return out\n",
    "\n",
    "def estimate_lag_by_frameidx(fidx_ref, fidx_cam, max_abs_shift=5_000):\n",
    "    \"\"\"Return shift (cam relative to ref) in frames using frame_idx.\"\"\"\n",
    "    if fidx_ref is None or fidx_cam is None:\n",
    "        return None\n",
    "    # Use median difference of overlapping indices\n",
    "    # Map frame_idx -> position\n",
    "    import numpy as np\n",
    "    ref_map = {int(v): i for i, v in enumerate(fidx_ref)}\n",
    "    inter = [(ref_map[int(v)], i) for i, v in enumerate(fidx_cam) if int(v) in ref_map]\n",
    "    if len(inter) < 100:\n",
    "        return None\n",
    "    diffs = [ri - ci for (ri, ci) in inter]\n",
    "    md = int(np.median(diffs))\n",
    "    if abs(md) > max_abs_shift:\n",
    "        return None\n",
    "    return md\n",
    "\n",
    "# Try frame_idx first\n",
    "lags = {ref_cam: 0}\n",
    "for cam in cams[1:]:\n",
    "    lag = estimate_lag_by_frameidx(per_cam_frameidx.get(ref_cam), per_cam_frameidx.get(cam))\n",
    "    lags[cam] = lag if lag is not None else 0\n",
    "\n",
    "# If any None, fall back to a quick search minimizing median pair_err on a few samples\n",
    "if any(l is None for l in lags.values()):\n",
    "    # Geometry helpers needed here:\n",
    "    Rts = {cam: np.linalg.inv(Ks[cam]) @ Ps[cam] for cam in Ps}\n",
    "    def undist_norm(xy, K, dist):\n",
    "        return cv2.undistortPoints(xy.reshape(1,1,2).astype(np.float64), K, dist, P=None).reshape(2)\n",
    "    def tri_2views_norm(Rt1, xn1, Rt2, xn2):\n",
    "        A = np.stack([xn1[0]*Rt1[2]-Rt1[0], xn1[1]*Rt1[2]-Rt1[1],\n",
    "                      xn2[0]*Rt2[2]-Rt2[0], xn2[1]*Rt2[2]-Rt2[1]], axis=0)\n",
    "        _,_,Vt = np.linalg.svd(A); Xh = Vt[-1]; Xh /= Xh[3]; return Xh[:3]\n",
    "    def undistort_px(xy, cam):\n",
    "        return cv2.undistortPoints(xy.reshape(1,1,2).astype(np.float64), Ks[cam], Dists[cam], P=Ks[cam]).reshape(2)\n",
    "    def pair_err(ref_xy, cam_xy, ref_cam, cam):\n",
    "        xn_ref = undist_norm(ref_xy, Ks[ref_cam], Dists[ref_cam])\n",
    "        xn_cam = undist_norm(cam_xy, Ks[cam], Dists[cam])\n",
    "        X = tri_2views_norm(Rts[ref_cam], xn_ref, Rts[cam], xn_cam)\n",
    "        u_ref = undistort_px(ref_xy, ref_cam)\n",
    "        u_cam = undistort_px(cam_xy, cam)\n",
    "        e_ref = np.linalg.norm((Ps[ref_cam] @ np.append(X,1.0))[:2] / (Ps[ref_cam] @ np.append(X,1.0))[2] - u_ref)\n",
    "        e_cam = np.linalg.norm((Ps[cam]      @ np.append(X,1.0))[:2] / (Ps[cam]      @ np.append(X,1.0))[2] - u_cam)\n",
    "        return 0.5*(e_ref+e_cam)\n",
    "\n",
    "    import numpy as np\n",
    "    Tref, N = per_cam_pts[ref_cam].shape[0], per_cam_pts[ref_cam].shape[1]\n",
    "    sample_t = np.linspace(0, Tref-1, num=min(400, Tref), dtype=int)\n",
    "    sample_j = list(range(min(N, 6)))  # 6 nodes max for speed\n",
    "    lag_window = int(0.5 * FPS)  # search ±0.5 s; increase if needed\n",
    "\n",
    "    for cam in cams[1:]:\n",
    "        if lags.get(cam) is not None:\n",
    "            continue\n",
    "        best_lag, best_med = 0, np.inf\n",
    "        for lag in range(-lag_window, lag_window+1, max(1,int(FPS//30))):  # step ~2 frames at 60fps\n",
    "            errs = []\n",
    "            for t in sample_t:\n",
    "                t2 = t - lag\n",
    "                if t2 < 0 or t2 >= per_cam_pts[cam].shape[0]:\n",
    "                    continue\n",
    "                for j in sample_j:\n",
    "                    xy1 = per_cam_pts[ref_cam][t, j, :]\n",
    "                    xy2 = per_cam_pts[cam][t2, j, :]\n",
    "                    if np.all(np.isfinite(xy1)) and np.all(np.isfinite(xy2)):\n",
    "                        try:\n",
    "                            errs.append(pair_err(xy1, xy2, ref_cam, cam))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "            if len(errs) > 50:\n",
    "                med = float(np.median(errs))\n",
    "                if med < best_med:\n",
    "                    best_med, best_lag = med, lag\n",
    "        lags[cam] = best_lag\n",
    "        print(f\"[align] estimated lag {cam} vs {ref_cam}: {best_lag} frames (median reproj err≈{best_med:.2f})\")\n",
    "\n",
    "# Apply lags and re-sync lengths\n",
    "for cam in cams:\n",
    "    per_cam_pts[cam] = shift_with_nans(per_cam_pts[cam], lags.get(cam, 0))\n",
    "\n",
    "lengths = {cam: per_cam_pts[cam].shape[0] for cam in per_cam_pts}\n",
    "T_common = min(lengths.values())\n",
    "per_cam_pts = {cam: per_cam_pts[cam][:T_common].copy() for cam in per_cam_pts}\n",
    "print(\"[align] applied lags:\", lags, \"→ new common T =\", T_common)\n",
    "\n",
    "# (then continue with your clean → triangulate steps)\n",
    "\n",
    "\n",
    "# ------------ CLEAN 2D + SYNC LENGTH -------------\n",
    "lengths = {cam: per_cam_pts[cam].shape[0] for cam in per_cam_pts}\n",
    "T_common = min(lengths.values())\n",
    "per_cam_pts = {cam: per_cam_pts[cam][:T_common].copy() for cam in per_cam_pts}\n",
    "\n",
    "def mark_outliers_speed(pts, max_px_per_frame=40):\n",
    "    ok = np.ones((pts.shape[0],), dtype=bool)\n",
    "    v = np.linalg.norm(np.diff(pts, axis=0), axis=1)\n",
    "    bad = np.r_[False, v > max_px_per_frame]\n",
    "    bad |= np.any(~np.isfinite(pts), axis=1)\n",
    "    ok[bad] = False\n",
    "    return ok\n",
    "\n",
    "def fill_short_gaps_2d(pts, max_gap=8):\n",
    "    out = pts.copy()\n",
    "    out[:, 0] = interp_short_nan_runs(pts[:, 0], max_gap)\n",
    "    out[:, 1] = interp_short_nan_runs(pts[:, 1], max_gap)\n",
    "    return out\n",
    "\n",
    "for cam in list(per_cam_pts.keys()):\n",
    "    arr = per_cam_pts[cam]\n",
    "    for j in range(arr.shape[1]):\n",
    "        ok = mark_outliers_speed(arr[:, j, :], max_px_per_frame=40)\n",
    "        arr[~ok, j, :] = np.nan\n",
    "        arr[:, j, :] = fill_short_gaps_2d(arr[:, j, :], max_gap=8)\n",
    "    per_cam_pts[cam] = arr\n",
    "    # #region agent log\n",
    "    valid_after = np.isfinite(arr).sum()\n",
    "    total_after = arr.size\n",
    "    debug_log(\"H1\", f\"clean_after_{cam}\", \"2D points validity after cleaning\", {\n",
    "        \"cam\": cam, \"valid_count\": int(valid_after), \"total_count\": int(total_after),\n",
    "        \"valid_fraction\": float(valid_after / total_after) if total_after > 0 else 0.0\n",
    "    })\n",
    "    # #endregion\n",
    "\n",
    "print(\"[clean] Per-cam lengths:\", lengths, \"→ using T =\", T_common)\n",
    "\n",
    "# ------------ GEOMETRY / UNDISTORT -------------\n",
    "Rts = {cam: np.linalg.inv(Ks[cam]) @ Ps[cam] for cam in Ps}\n",
    "\n",
    "def undist_norm(xy, K, dist):\n",
    "    return cv2.undistortPoints(xy.reshape(1, 1, 2).astype(np.float64), K, dist, P=None).reshape(2)\n",
    "\n",
    "def tri_2views_norm(Rt1, xn1, Rt2, xn2):\n",
    "    A = np.stack([\n",
    "        xn1[0] * Rt1[2] - Rt1[0],\n",
    "        xn1[1] * Rt1[2] - Rt1[1],\n",
    "        xn2[0] * Rt2[2] - Rt2[0],\n",
    "        xn2[1] * Rt2[2] - Rt2[1],\n",
    "    ], axis=0)\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    Xh = Vt[-1]; Xh /= Xh[3]\n",
    "    return Xh[:3]\n",
    "\n",
    "def proj_px(cam, X):\n",
    "    x = Ps[cam] @ np.append(X, 1.0)\n",
    "    return (x[:2] / x[2]).astype(float)\n",
    "\n",
    "def undistort_px(xy, cam):\n",
    "    return cv2.undistortPoints(xy.reshape(1, 1, 2).astype(np.float64), Ks[cam], Dists[cam], P=Ks[cam]).reshape(2)\n",
    "\n",
    "def pair_err(ref_xy, cam_xy, ref_cam, cam):\n",
    "    try:\n",
    "        xn_ref = undist_norm(ref_xy, Ks[ref_cam], Dists[ref_cam])\n",
    "        xn_cam = undist_norm(cam_xy, Ks[cam], Dists[cam])\n",
    "        X = tri_2views_norm(Rts[ref_cam], xn_ref, Rts[cam], xn_cam)\n",
    "        u_ref = undistort_px(ref_xy, ref_cam)\n",
    "        u_cam = undistort_px(cam_xy, cam)\n",
    "        e_ref = np.linalg.norm(proj_px(ref_cam, X) - u_ref)\n",
    "        e_cam = np.linalg.norm(proj_px(cam, X) - u_cam)\n",
    "        err = 0.5 * (e_ref + e_cam)\n",
    "        # #region agent log\n",
    "        if not np.isfinite(err) or err > 1000:\n",
    "            debug_log(\"H2\", \"pair_err\", \"High or invalid reprojection error\", {\n",
    "                \"ref_cam\": ref_cam, \"cam\": cam, \"ref_xy\": [float(ref_xy[0]), float(ref_xy[1])],\n",
    "                \"cam_xy\": [float(cam_xy[0]), float(cam_xy[1])], \"error\": float(err) if np.isfinite(err) else None,\n",
    "                \"e_ref\": float(e_ref) if np.isfinite(e_ref) else None, \"e_cam\": float(e_cam) if np.isfinite(e_cam) else None\n",
    "            })\n",
    "        # #endregion\n",
    "        return err\n",
    "    except Exception as ex:\n",
    "        # #region agent log\n",
    "        debug_log(\"H2\", \"pair_err_exception\", \"Exception in pair_err\", {\n",
    "            \"ref_cam\": ref_cam, \"cam\": cam, \"ref_xy\": [float(ref_xy[0]), float(ref_xy[1])] if np.all(np.isfinite(ref_xy)) else None,\n",
    "            \"cam_xy\": [float(cam_xy[0]), float(cam_xy[1])] if np.all(np.isfinite(cam_xy)) else None,\n",
    "            \"exception\": str(type(ex).__name__), \"message\": str(ex)\n",
    "        })\n",
    "        # #endregion\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "# ------------ ALIGN NODE ORDER ACROSS CAMERAS (by geometry) -------------\n",
    "def estimate_node_permutation_to_ref(ref_cam, cam, sample_frames, min_pairs=60, bad_cost=1e6):\n",
    "    \"\"\"\n",
    "    Returns an array perm of length N where:\n",
    "      per_cam_pts[cam][:, perm[i], :] ≈ per_cam_pts[ref_cam][:, i, :]\n",
    "    i.e., perm maps each REF index i to the best matching OLD index in cam.\n",
    "    \"\"\"\n",
    "    T, N = per_cam_pts[ref_cam].shape[0], per_cam_pts[ref_cam].shape[1]\n",
    "    C = np.full((N, N), bad_cost, dtype=float)  # cost matrix: ref-index i vs cam-index j\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            errs = []\n",
    "            exception_count = 0\n",
    "            for t in sample_frames:\n",
    "                if t < 0 or t >= T or t >= per_cam_pts[cam].shape[0]:\n",
    "                    continue\n",
    "                xy1 = per_cam_pts[ref_cam][t, i, :]\n",
    "                xy2 = per_cam_pts[cam][t, j, :]\n",
    "                if np.all(np.isfinite(xy1)) and np.all(np.isfinite(xy2)):\n",
    "                    try:\n",
    "                        errs.append(pair_err(xy1, xy2, ref_cam, cam))\n",
    "                    except Exception as ex:\n",
    "                        exception_count += 1\n",
    "                        pass\n",
    "            if len(errs) >= min_pairs:\n",
    "                C[i, j] = float(np.median(errs))\n",
    "            # #region agent log\n",
    "            if i == 0 and j == 0:  # Log first pair as sample\n",
    "                debug_log(\"H2\", f\"node_match_{cam}\", \"Node matching sample\", {\n",
    "                    \"ref_cam\": ref_cam, \"cam\": cam, \"ref_node\": i, \"cam_node\": j,\n",
    "                    \"valid_pairs\": len(errs), \"exception_count\": exception_count,\n",
    "                    \"median_err\": float(np.median(errs)) if len(errs) >= min_pairs else None,\n",
    "                    \"cost\": float(C[i, j]) if len(errs) >= min_pairs else None\n",
    "                })\n",
    "            # #endregion\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(C)  # row=ref i, col=cam j\n",
    "    perm = np.array(col_ind, dtype=int)          # perm[i] = old cam index that matches ref index i\n",
    "    med_costs = C[row_ind, col_ind]\n",
    "    return perm, med_costs, C\n",
    "\n",
    "# choose sample frames for robust stats\n",
    "T_ref = per_cam_pts[ref_cam].shape[0]\n",
    "sample_frames = np.linspace(0, T_ref-1, num=min(800, T_ref), dtype=int)\n",
    "\n",
    "# run for each non-ref cam, then reorder its node axis\n",
    "for cam in cams:\n",
    "    if cam == ref_cam:\n",
    "        continue\n",
    "    perm, costs, Cmat = estimate_node_permutation_to_ref(ref_cam, cam, sample_frames)\n",
    "    # simple sanity: if most assigned costs are huge, warn but still apply (can loosen thresholds here)\n",
    "    n_bad = int(np.sum(~np.isfinite(costs)) + np.sum(costs > 80.0))  # 80 px median reproj err ~ bad\n",
    "    print(f\"[match] {cam} → {ref_cam}: median assigned costs (first 10) = {np.round(costs[:10], 1)}\")\n",
    "    if n_bad > len(costs)//2:\n",
    "        print(f\"[match][warn] many high costs. Check calibration or node consistency.\")\n",
    "    # reorder the camera's node dimension to align with reference indices\n",
    "    per_cam_pts[cam] = per_cam_pts[cam][:, perm, :]\n",
    "print(\"[match] aligned node ordering across cameras using geometric consistency.\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------ AUTO U/L SWAP CHECK (2D) -------------\n",
    "rng = default_rng(0)\n",
    "cams = list(per_cam_pts.keys())\n",
    "ref_cam = cams[0]\n",
    "T = per_cam_pts[ref_cam].shape[0]\n",
    "sample_idx = np.arange(0, T, max(1, T // 300))\n",
    "\n",
    "# try to find U/L by name if present (for swap check only)\n",
    "idxU_guess, idxL_guess, _ = find_alias_indices(node_names_ref, ALIAS)\n",
    "\n",
    "swap_votes, total_votes = {c: 0 for c in cams}, {c: 0 for c in cams}\n",
    "if idxU_guess is not None and idxL_guess is not None:\n",
    "    for cam in cams:\n",
    "        if cam == ref_cam: \n",
    "            continue\n",
    "        vs, vt = 0, 0\n",
    "        for t in sample_idx:\n",
    "            refU = per_cam_pts[ref_cam][t, idxU_guess, :]\n",
    "            refL = per_cam_pts[ref_cam][t, idxL_guess, :]\n",
    "            camU = per_cam_pts[cam][t, idxU_guess, :]\n",
    "            camL = per_cam_pts[cam][t, idxL_guess, :]\n",
    "            if not (np.all(np.isfinite(refU)) and np.all(np.isfinite(refL)) and\n",
    "                    np.all(np.isfinite(camU)) and np.all(np.isfinite(camL))):\n",
    "                continue\n",
    "            e0 = pair_err(refU, camU, ref_cam, cam) + pair_err(refL, camL, ref_cam, cam)\n",
    "            e1 = pair_err(refU, camL, ref_cam, cam) + pair_err(refL, camU, ref_cam, cam)\n",
    "            vt += 1\n",
    "            if e1 < e0: \n",
    "                vs += 1\n",
    "        swap_votes[cam] = vs; total_votes[cam] = vt\n",
    "\n",
    "    cams_to_swap = [c for c in cams if c != ref_cam and total_votes[c] > 0 and (swap_votes[c] / total_votes[c]) > 0.65]\n",
    "    print(\"[swap] votes swap/total:\", {c: f\"{swap_votes[c]}/{total_votes[c]}\" for c in cams})\n",
    "    print(\"[swap] applying to:\", cams_to_swap)\n",
    "    for cam in cams_to_swap:\n",
    "        arr = per_cam_pts[cam]\n",
    "        arr[:, [idxU_guess, idxL_guess], :] = arr[:, [idxL_guess, idxU_guess], :]\n",
    "        per_cam_pts[cam] = arr\n",
    "else:\n",
    "    print(\"[swap] U/L aliases not found → skipping 2D swap check.\")\n",
    "\n",
    "# ------------ TRIANGULATE ALL NODES -------------\n",
    "def tri_2views(P1, xy1, P2, xy2):\n",
    "    A = np.stack([\n",
    "        xy1[0] * P1[2, :] - P1[0, :],\n",
    "        xy1[1] * P1[2, :] - P1[1, :],\n",
    "        xy2[0] * P2[2, :] - P2[0, :],\n",
    "        xy2[1] * P2[2, :] - P2[1, :],\n",
    "    ], axis=0)\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    X = Vt[-1, :]\n",
    "    X /= X[3]\n",
    "    return X[:3]\n",
    "\n",
    "def reproj_err(P, X, xy):\n",
    "    x = P @ np.append(X, 1.0)\n",
    "    x = x[:2] / x[2]\n",
    "    return float(np.linalg.norm(x - xy))\n",
    "\n",
    "def best_pair_at_t(j, t):\n",
    "    best = None  # (err, (c1,c2), X)\n",
    "    valid_pairs = 0\n",
    "    exception_count = 0\n",
    "    for c1, c2 in combinations(cams, 2):\n",
    "        xy1 = per_cam_pts[c1][t, j, :]\n",
    "        xy2 = per_cam_pts[c2][t, j, :]\n",
    "        if not (np.all(np.isfinite(xy1)) and np.all(np.isfinite(xy2))):\n",
    "            continue\n",
    "        try:\n",
    "            u1 = undistort_px(xy1, c1)\n",
    "            u2 = undistort_px(xy2, c2)\n",
    "            X  = tri_2views(Ps[c1], u1, Ps[c2], u2)\n",
    "            e1 = reproj_err(Ps[c1], X, u1)\n",
    "            e2 = reproj_err(Ps[c2], X, u2)\n",
    "            e  = 0.5 * (e1 + e2)\n",
    "            valid_pairs += 1\n",
    "            if (best is None) or (e < best[0]):\n",
    "                best = (e, (c1, c2), X)\n",
    "        except Exception as ex:\n",
    "            exception_count += 1\n",
    "            # #region agent log\n",
    "            if j == 0 and t < 10:  # Log first few as sample\n",
    "                debug_log(\"H4\", \"triangulate_exception\", \"Exception during triangulation\", {\n",
    "                    \"node\": j, \"frame\": t, \"c1\": c1, \"c2\": c2,\n",
    "                    \"xy1\": [float(xy1[0]), float(xy1[1])], \"xy2\": [float(xy2[0]), float(xy2[1])],\n",
    "                    \"exception\": str(type(ex).__name__), \"message\": str(ex)\n",
    "                })\n",
    "            # #endregion\n",
    "            continue\n",
    "    # #region agent log\n",
    "    if j == 0 and t < 10:  # Log first few frames as sample\n",
    "        debug_log(\"H4\", \"triangulate_sample\", \"Triangulation attempt\", {\n",
    "            \"node\": j, \"frame\": t, \"valid_pairs\": valid_pairs, \"exception_count\": exception_count,\n",
    "            \"best_error\": float(best[0]) if best is not None else None,\n",
    "            \"best_cams\": list(best[1]) if best is not None else None\n",
    "        })\n",
    "    # #endregion\n",
    "    return best\n",
    "\n",
    "REPROJ_STRICT = 10.0\n",
    "REPROJ_LOOSE  = 18.0\n",
    "\n",
    "N = per_cam_pts[ref_cam].shape[1]\n",
    "X3 = np.full((T, N, 3), np.nan, dtype=float)\n",
    "accept_counts = np.zeros(N, dtype=int)\n",
    "\n",
    "for j in range(N):\n",
    "    prev_good = False\n",
    "    total_attempts = 0\n",
    "    accepted = 0\n",
    "    rejected_strict = 0\n",
    "    rejected_loose = 0\n",
    "    for t in range(T):\n",
    "        bp = best_pair_at_t(j, t)\n",
    "        if bp is None:\n",
    "            prev_good = False\n",
    "            continue\n",
    "        total_attempts += 1\n",
    "        e, _, X = bp\n",
    "        if e <= REPROJ_STRICT or (e <= REPROJ_LOOSE and prev_good):\n",
    "            X3[t, j, :] = X\n",
    "            accept_counts[j] += 1\n",
    "            accepted += 1\n",
    "            prev_good = True\n",
    "        else:\n",
    "            if e > REPROJ_STRICT and not prev_good:\n",
    "                rejected_strict += 1\n",
    "            else:\n",
    "                rejected_loose += 1\n",
    "            prev_good = False\n",
    "    # #region agent log\n",
    "    if j < 3:  # Log first 3 nodes\n",
    "        debug_log(\"H4\", f\"triangulate_node_{j}\", \"Triangulation summary for node\", {\n",
    "            \"node\": j, \"total_attempts\": total_attempts, \"accepted\": accepted,\n",
    "            \"rejected_strict\": rejected_strict, \"rejected_loose\": rejected_loose,\n",
    "            \"accept_rate\": float(accepted / total_attempts) if total_attempts > 0 else 0.0\n",
    "        })\n",
    "    # #endregion\n",
    "\n",
    "# short gap fill (≤0.3 s)\n",
    "MAX_GAP_3D = int(0.30 * FPS)\n",
    "for j in range(N):\n",
    "    for d in range(3):\n",
    "        X3[:, j, d] = interp_short_nan_runs(X3[:, j, d], MAX_GAP_3D)\n",
    "\n",
    "cov_by_node = (np.isfinite(X3).all(axis=2)).mean(axis=0)\n",
    "print(\"[tri+fill] per-node coverage (first 10):\", np.round(cov_by_node[:10], 3))\n",
    "\n",
    "# ------------ SAVE ALL NODES 3D -------------\n",
    "all_npz = OUTDIR / f\"all_nodes_3d_{SUFFIX}.npz\"\n",
    "all_csv = OUTDIR / f\"all_nodes_3d_long_{SUFFIX}.csv\"\n",
    "np.savez_compressed(str(all_npz), X3=X3.astype(np.float32), node_names=np.array(node_names_ref), FPS=np.float32(FPS))\n",
    "log(f\"[OK] wrote all nodes 3D arrays: {all_npz}\")\n",
    "\n",
    "# long-form csv\n",
    "rows = []\n",
    "tvec_full = np.arange(T) / FPS\n",
    "for j, name in enumerate(node_names_ref):\n",
    "    rows.append(pd.DataFrame({\n",
    "        \"frame\": np.arange(T),\n",
    "        \"time_s\": tvec_full,\n",
    "        \"node\": name,\n",
    "        \"x\": X3[:, j, 0], \"y\": X3[:, j, 1], \"z\": X3[:, j, 2]\n",
    "    }))\n",
    "pd.concat(rows, ignore_index=True).to_csv(all_csv, index=False)\n",
    "log(f\"[OK] wrote all nodes 3D long CSV: {all_csv}\")\n",
    "\n",
    "# ------------ GAPE PAIR SELECTION -------------\n",
    "def pick_gape_pair(node_names, X3, FPS):\n",
    "    \"\"\"Return (idxU, idxL) or (None,None).\n",
    "       Strategy:\n",
    "       1) Use aliases if both U and L found.\n",
    "       2) Else prefer names containing MOUTHY substrings; if none, use all.\n",
    "       3) Score every pair by robust amplitude × coverage; return best.\n",
    "    \"\"\"\n",
    "    idxU, idxL, _ = find_alias_indices(node_names, ALIAS)\n",
    "    if idxU is not None and idxL is not None:\n",
    "        return idxU, idxL\n",
    "\n",
    "    # candidate index list\n",
    "    cand = [i for i,n in enumerate(node_names) if name_matches_any(n, MOUTHY)]\n",
    "    if len(cand) < 2:\n",
    "        cand = list(range(len(node_names)))  # fallback: all nodes\n",
    "\n",
    "    cov = np.isfinite(X3).all(axis=2)  # (T,N)\n",
    "    best_pair, best_score = (None, None), -np.inf\n",
    "    Tloc = X3.shape[0]\n",
    "\n",
    "    # Precompute distances efficiently by streaming pairs\n",
    "    for i, j in combinations(cand, 2):\n",
    "        good = cov[:, i] & cov[:, j]\n",
    "        if np.mean(good) < 0.5:\n",
    "            continue\n",
    "        d = np.full(Tloc, np.nan, dtype=float)\n",
    "        dd = X3[:, i, :] - X3[:, j, :]\n",
    "        g = good.nonzero()[0]\n",
    "        if g.size > 0:\n",
    "            d[g] = np.linalg.norm(dd[g, :], axis=1)\n",
    "        # robust amplitude\n",
    "        seg = d[np.isfinite(d)]\n",
    "        if seg.size < int(1.0 * FPS):\n",
    "            continue\n",
    "        q10, q90 = np.nanpercentile(seg, [10, 90])\n",
    "        amp = max(0.0, q90 - q10)\n",
    "        covg = np.mean(np.isfinite(d))\n",
    "        score = amp * (0.5 + 0.5 * covg)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_pair = (i, j)\n",
    "\n",
    "    if best_pair[0] is None:\n",
    "        return None, None\n",
    "    # arbitrary order: label the one with higher median Z as \"U\" (often higher in camera world),\n",
    "    # but this is arbitrary; downstream treats them symmetrically.\n",
    "    i, j = best_pair\n",
    "    zi = np.nanmedian(X3[:, i, 2]); zj = np.nanmedian(X3[:, j, 2])\n",
    "    return (i, j) if zi >= zj else (j, i)\n",
    "\n",
    "# ------------ GAPE PIPELINE (only if a pair is found) -------------\n",
    "idxU, idxL = pick_gape_pair(node_names_ref, X3, FPS)\n",
    "if idxU is not None and idxL is not None:\n",
    "    print(f\"[gape] using nodes: U={node_names_ref[idxU]}  L={node_names_ref[idxL]}  (indices {idxU},{idxL})\")\n",
    "    U3 = X3[:, idxU, :]\n",
    "    L3 = X3[:, idxL, :]\n",
    "\n",
    "    # save the chosen pair\n",
    "    tri_out_csv = OUTDIR / f\"triangulated_UL_3d_{SUFFIX}.csv\"\n",
    "    tri_out_npz = OUTDIR / f\"triangulated_UL_3d_{SUFFIX}.npz\"\n",
    "    pd.DataFrame({\n",
    "        \"frame\": np.arange(T),\n",
    "        \"time_s\": np.arange(T)/FPS,\n",
    "        \"Ux\": U3[:,0], \"Uy\": U3[:,1], \"Uz\": U3[:,2],\n",
    "        \"Lx\": L3[:,0], \"Ly\": L3[:,1], \"Lz\": L3[:,2],\n",
    "    }).to_csv(tri_out_csv, index=False)\n",
    "    np.savez_compressed(str(tri_out_npz), U3=U3.astype(np.float32), L3=L3.astype(np.float32), FPS=np.float32(FPS))\n",
    "    log(f\"[OK] wrote UL 3D tracks: {tri_out_csv}, {tri_out_npz}\")\n",
    "\n",
    "    # ---- Gape compute (meters) ----\n",
    "    def odd_leq(n): \n",
    "        return max(3, n if n % 2 else n - 1)\n",
    "\n",
    "    gape = np.full(T, np.nan)\n",
    "    good3d = np.all(np.isfinite(U3), 1) & np.all(np.isfinite(L3), 1)\n",
    "    gape[good3d] = np.linalg.norm(U3[good3d] - L3[good3d], axis=1)\n",
    "\n",
    "    # Hampel + speed cap\n",
    "    def hampel_1d(x, win, k=3.0):\n",
    "        x = x.copy(); n = len(x); h = max(3, win//2)\n",
    "        for i in range(n):\n",
    "            j0, j1 = max(0, i-h), min(n, i+h+1)\n",
    "            s = x[j0:j1]; s = s[np.isfinite(s)]\n",
    "            if s.size < 5 or not np.isfinite(x[i]): \n",
    "                continue\n",
    "            med = np.median(s); mad = np.median(np.abs(s - med)) + 1e-9\n",
    "            if abs(x[i] - med) > k * 1.4826 * mad:\n",
    "                x[i] = np.nan\n",
    "        return x\n",
    "\n",
    "    gape = hampel_1d(gape, int(0.30 * FPS))\n",
    "    gape_mm = gape * 1000.0\n",
    "    dg = np.gradient(gape_mm, 1.0 / FPS)\n",
    "    gape_mm[np.abs(dg) > 350.0] = np.nan\n",
    "    gape = interp_short_nan_runs(gape, int(0.25 * FPS))\n",
    "\n",
    "    PRE_W   = odd_leq(int(0.18 * FPS))\n",
    "    BASE_W  = odd_leq(int(2.0  * FPS))\n",
    "    g_pre   = savgol_filter(np.nan_to_num(gape, nan=np.nanmedian(gape)), PRE_W, 2, mode=\"interp\")\n",
    "    baseline = pd.Series(g_pre).rolling(BASE_W, center=True, \n",
    "                                        min_periods=max(3, BASE_W//3)).min().to_numpy()\n",
    "    gape_zero = np.clip(gape - baseline, 0.0, None)\n",
    "    gape_zero_mm = gape_zero * 1000.0\n",
    "    gape_zero_mm[gape_zero_mm > 50.0] = np.nan\n",
    "    gape_zero_mm = interp_short_nan_runs(gape_zero_mm, int(0.25 * FPS))\n",
    "\n",
    "    SMOOTH_W = odd_leq(int(0.22 * FPS))\n",
    "    gape_s_mm = savgol_filter(np.nan_to_num(gape_zero_mm, nan=np.nanmedian(gape_zero_mm)),\n",
    "                              SMOOTH_W, 3, mode=\"interp\")\n",
    "    gape_s = gape_s_mm / 1000.0\n",
    "    acc_s  = savgol_filter(gape_s, SMOOTH_W, 3, deriv=2, delta=1.0/FPS, mode=\"interp\")\n",
    "\n",
    "    q = np.nanpercentile(gape_s_mm, [0, 25, 50, 75, 95])\n",
    "    print(f\"[debug] gape_s_mm percentiles: min {q[0]:.1f}, p25 {q[1]:.1f}, med {q[2]:.1f}, p75 {q[3]:.1f}, p95 {q[4]:.1f}\")\n",
    "\n",
    "    # best 10s window\n",
    "    WSEC = 10.0\n",
    "    win = int(WSEC * FPS)\n",
    "    stride = max(1, int(0.05 * FPS))\n",
    "    def robust_amp_mm(xmm):\n",
    "        q10, q90 = np.nanpercentile(xmm, [10, 90])\n",
    "        return q90 - q10\n",
    "\n",
    "    best_f0, best_score = 0, -np.inf\n",
    "    for f0 in range(0, max(1, T - win + 1), stride):\n",
    "        seg = gape_s_mm[f0:f0+win]\n",
    "        cov = np.mean(np.isfinite(seg))\n",
    "        if cov < 0.85: \n",
    "            continue\n",
    "        sc = robust_amp_mm(seg) * (0.6 + 0.4*cov)\n",
    "        if sc > best_score:\n",
    "            best_score, best_f0 = sc, f0\n",
    "\n",
    "    f0, f1 = best_f0, min(best_f0 + win, T)\n",
    "    tvec = np.arange(f0, f1) / FPS\n",
    "\n",
    "    # plots\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 3.2))\n",
    "    ax1.plot(tvec, gape_s_mm[f0:f1], label=\"Gape (smoothed)\")\n",
    "    ax1.set_xlabel(\"Time (s)\"); ax1.set_ylabel(\"Gape (mm)\"); ax1.set_ylim(bottom=0, top=60)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(tvec, acc_s[f0:f1]*1000.0, color=\"orange\", label=\"Acceleration\")\n",
    "    ax2.set_ylabel(\"Acceleration (mm/s²)\", color=\"orange\")\n",
    "    h1,l1 = ax1.get_legend_handles_labels(); h2,l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=\"upper right\")\n",
    "    ax1.set_title(\"Vertical gape (zero-baseline) & acceleration — best 10 s\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(OUTDIR / f\"gape_10s_overlay_zero_clean_{SUFFIX}.png\")); plt.close()\n",
    "\n",
    "    seg_mm = gape_s_mm[f0:f1]\n",
    "    pks,_ = find_peaks(seg_mm, distance=max(1, int(0.06*FPS)))\n",
    "    trs,_ = find_peaks(-seg_mm, distance=max(1, int(0.06*FPS)))\n",
    "    fig, ax = plt.subplots(figsize=(10, 2.8))\n",
    "    ax.plot(tvec, seg_mm, lw=1.4)\n",
    "    ax.scatter(tvec[pks], seg_mm[pks], s=28, marker=\"o\", label=\"peaks\")\n",
    "    ax.scatter(tvec[trs], seg_mm[trs], s=28, marker=\"v\", label=\"troughs\")\n",
    "    ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Gape (mm)\"); ax.set_ylim(bottom=0, top=60)\n",
    "    ax.set_title(\"Gape — detected cycle landmarks (10 s)\")\n",
    "    ax.legend(); plt.tight_layout()\n",
    "    plt.savefig(str(OUTDIR / f\"gape_10s_cycles_zero_clean_{SUFFIX}.png\")); plt.close()\n",
    "\n",
    "    # 30–40 s panel if available\n",
    "    if T / FPS > 40.0:\n",
    "        f0L, f1L = int(30*FPS), int(40*FPS)\n",
    "        tt = np.arange(f0L, f1L) / FPS\n",
    "        fig, ax1 = plt.subplots(figsize=(16, 3.2))\n",
    "        ax1.plot(tt, gape_s_mm[f0L:f1L], label=\"Gape (smoothed)\")\n",
    "        ax1.set_xlabel(\"time (s)\"); ax1.set_ylabel(\"gape (mm)\")\n",
    "        ax1.set_ylim(bottom=0, top=60)\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(tt, acc_s[f0L:f1L]*1000.0, color=\"orange\", label=\"Acceleration\")\n",
    "        ax2.set_ylabel(\"acceleration (mm/s²)\", color=\"orange\")\n",
    "        h1,l1 = ax1.get_legend_handles_labels(); h2,l2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(h1+h2, l1+l2, loc=\"upper right\")\n",
    "        ax1.set_title(\"Vertical gape displacement & acceleration — 30–40 s\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(OUTDIR / f\"gape_30to40s_overlay_zero_clean_{SUFFIX}.png\")); plt.close()\n",
    "\n",
    "    # export time series\n",
    "    pd.DataFrame({\n",
    "        \"frame\": np.arange(T),\n",
    "        \"time_s\": np.arange(T)/FPS,\n",
    "        \"gape_m_raw\": gape,\n",
    "        \"gape_m_zero\": gape_s_mm/1000.0,\n",
    "        \"gape_mm_zero\": gape_s_mm,\n",
    "        \"acc_m_per_s2\": acc_s\n",
    "    }).to_csv(OUTDIR / f\"gape_timeseries_from_sleap_zero_{SUFFIX}.csv\", index=False)\n",
    "\n",
    "    log(\"[DONE] Wrote gape time-series and plots.\")\n",
    "else:\n",
    "    log(\"[info] Could not identify a reliable U/L pair → skipped gape. 3D for ALL nodes was saved.\")\n",
    "\n",
    "log(\"Phase 0 triangulation complete.\")\n",
    "\n",
    "# === End ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b46644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0: 2D → 3D Triangulation (batch runner)\n",
    "# Aligned with Phase 1 style; outputs written to face/results/n20/pipeline/phase0/\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, h5py, re, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "from numpy.random import default_rng\n",
    "import cv2\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "BASE_DIR = Path(\"/Users/howardwang/Desktop/Ruten/Evaluation-Metrics_Vishal-main\")\n",
    "SESSION = \"2025-05-28_14-12-04_124591\"\n",
    "OUTPUT_TAG = \"v1\"\n",
    "CALIB_JSON = BASE_DIR / \"Calibration/calibration.json\"\n",
    "WORKDIR = BASE_DIR / \"face/results/n20/pipeline/phase0\" / f\"session_{SESSION}_{OUTPUT_TAG}\" / \"work\"\n",
    "OUTDIR = WORKDIR.parent\n",
    "LOG_PATH = OUTDIR / \"pipeline_log.txt\"\n",
    "\n",
    "CAM_ORDER = [\"cam-topright\", \"cam-topleft\", \"cam-bottomright\", \"cam-bottomleft\"]\n",
    "PRED_FILES = {cam: WORKDIR / f\"{cam}.analysis.h5\" for cam in CAM_ORDER}\n",
    "\n",
    "FPS_FALLBACK = 120.0\n",
    "MAX_SPEED_PX = 40\n",
    "MAX_GAP_2D = 8\n",
    "MAX_GAP_3D_SEC = 0.30\n",
    "REPROJ_STRICT = 10.0\n",
    "REPROJ_LOOSE = 18.0\n",
    "LAG_WINDOW_SEC = 0.5\n",
    "SAMPLE_FRAMES_ALIGN = 800\n",
    "SAMPLE_NODES_ALIGN = 6\n",
    "SUFFIX = OUTPUT_TAG\n",
    "\n",
    "# ------------------ LOGGER ------------------\n",
    "def log(msg):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts}] {msg}\"\n",
    "    print(line)\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
    "log(\"Phase 0 triangulation start\")\n",
    "log(f\"Session={SESSION}, OUTDIR={OUTDIR}\")\n",
    "\n",
    "# ------------------ ALIASES ------------------\n",
    "ALIAS = {\n",
    "    \"U\": [\"u\",\"upper\",\"upper_lip\",\"upperlip\",\"u_star\",\"upperlipmarker\",\"toplip\",\"top_lip\",\"lip_u\",\"ulip\"],\n",
    "    \"L\": [\"l\",\"lower\",\"lower_lip\",\"lowerlip\",\"l_star\",\"lowerlipmarker\",\"bottomlip\",\"bot_lip\",\"lip_l\",\"llip\"],\n",
    "    \"HEAD\": [\"head\",\"snout\",\"nose\",\"forehead\",\"ha\",\"hb\",\"hc\"],\n",
    "}\n",
    "MOUTHY = [\"lip\",\"mouth\",\"jaw\",\"mandible\",\"maxilla\",\"muzzle\"]\n",
    "\n",
    "# ------------------ HELPERS ------------------\n",
    "def interp_short_nan_runs(y, max_gap):\n",
    "    y = y.copy(); n = len(y); idx = np.arange(n)\n",
    "    good = np.isfinite(y)\n",
    "    if good.sum() < 2:\n",
    "        return y\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if good[i]:\n",
    "            i += 1; continue\n",
    "        j = i\n",
    "        while j < n and not good[j]:\n",
    "            j += 1\n",
    "        gap = j - i\n",
    "        if 0 < gap <= max_gap and i > 0 and j < n and good[i-1] and good[j]:\n",
    "            y[i:j] = np.interp(idx[i:j], [i-1, j], [y[i-1], y[j]])\n",
    "            good[i:j] = True\n",
    "        i = j\n",
    "    return y\n",
    "\n",
    "\n",
    "def load_sleap_h5(path):\n",
    "    def _extract_fps(f):\n",
    "        fps = None\n",
    "        for k in [\"video_fps\",\"videos/fps\",\"video/fps\"]:\n",
    "            if k in f:\n",
    "                v = f[k][()]; fps = float(v[0] if np.size(v) else v); break\n",
    "        return fps\n",
    "    def _extract_names(f, N):\n",
    "        for k in [\"node_names\",\"nodes\",\"points/labels\",\"tracks/labels\",\"labels\"]:\n",
    "            if k in f:\n",
    "                try:\n",
    "                    nn = [x.decode(\"utf-8\") if isinstance(x, bytes) else str(x) for x in np.array(f[k]).ravel()]\n",
    "                    return nn\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return [f\"node_{i+1}\" for i in range(N)]\n",
    "    def _try_classic(f):\n",
    "        best = None\n",
    "        def walk(g, prefix=\"\"):\n",
    "            nonlocal best\n",
    "            for k, v in g.items():\n",
    "                p = f\"{prefix}/{k}\" if prefix else f\"/{k}\"\n",
    "                if isinstance(v, h5py.Group):\n",
    "                    walk(v, p)\n",
    "                else:\n",
    "                    if v.ndim in (3,4) and v.shape[-1] == 2 and np.issubdtype(v.dtype, np.number):\n",
    "                        s = 0\n",
    "                        if v.ndim == 4: s += 3\n",
    "                        if v.shape[-1] == 2: s += 2\n",
    "                        if max(v.shape) > 50: s += 1\n",
    "                        if best is None or s > best[0]:\n",
    "                            best = (s, p, tuple(v.shape))\n",
    "        walk(f)\n",
    "        if not best:\n",
    "            return None\n",
    "        ds = np.array(f[best[1]])\n",
    "        if ds.ndim == 4:\n",
    "            s = ds.shape\n",
    "            if s[1] == 2 and s[-1] > 50:          ds = ds.transpose(0,3,2,1)\n",
    "            elif s[2] == 2 and s[0] > 50:         ds = ds.transpose(3,0,1,2)\n",
    "            elif s[0] == 2:                        ds = ds.transpose(3,2,1,0)\n",
    "            elif s[-1] != 2:\n",
    "                dims = list(s); coord_ax = int(np.where(np.array(dims)==2)[0][0])\n",
    "                frame_ax = int(np.argmax(dims))\n",
    "                candidates = [i for i in range(4) if i != coord_ax]\n",
    "                k_ax = candidates[int(np.argmin([dims[i] for i in candidates]))]\n",
    "                n_ax = [i for i in range(4) if i not in (coord_ax, frame_ax, k_ax)][0]\n",
    "                ds = ds.transpose(k_ax, frame_ax, n_ax, coord_ax)\n",
    "        elif ds.ndim == 3 and ds.shape[-1] == 2:\n",
    "            ds = ds[None, ...]\n",
    "        else:\n",
    "            return None\n",
    "        K, T, N, _ = ds.shape\n",
    "        kbest = 0 if K == 1 else int(np.argmax(np.sum(np.isfinite(ds).all(axis=-1), axis=(1,2))))\n",
    "        pts = ds[kbest].astype(float)\n",
    "        names = _extract_names(f, N)\n",
    "        fps = _extract_fps(f)\n",
    "        def find_score_key():\n",
    "            keys = [\"point_scores\",\"points_scores\",\"node_scores\",\"point_confidences\",\"scores\"]\n",
    "            for k in keys:\n",
    "                if k in f: return k\n",
    "            for grp in f.keys():\n",
    "                if isinstance(f[grp], h5py.Group):\n",
    "                    for k in keys:\n",
    "                        if k in f[grp]: return f\"{grp}/{k}\"\n",
    "            return None\n",
    "        sk = find_score_key()\n",
    "        if sk is not None:\n",
    "            sc = np.array(f[sk]).squeeze()\n",
    "            TT, NN = pts.shape[0], pts.shape[1]\n",
    "            if sc.ndim == 3 and sc.shape[-1] in (1,2): sc = sc[...,0]\n",
    "            elif sc.ndim == 2 and sc.shape == (NN,TT): sc = sc.T\n",
    "            elif sc.ndim == 1 and sc.shape[0] == NN:   sc = np.tile(sc[None,:], (TT,1))\n",
    "            elif sc.ndim == 1 and sc.shape[0] == TT:   sc = np.tile(sc[:,None], (1,NN))\n",
    "            elif sc.ndim != 2:                         sc = None\n",
    "            if sc is not None:\n",
    "                bad = ~np.isfinite(sc) | (sc <= 0)\n",
    "                pts[bad] = np.nan\n",
    "        return pts, names, fps, None\n",
    "    def _try_flat_predictions(f):\n",
    "        if \"predictions\" not in f: return None\n",
    "        g = f[\"predictions\"]\n",
    "        pat = re.compile(r\"^(\\d+)\\.(x|y|score)$\")\n",
    "        nodes = set()\n",
    "        for k in g.keys():\n",
    "            m = pat.match(k)\n",
    "            if m and m.group(2) in (\"x\",\"y\"):\n",
    "                nodes.add(int(m.group(1)))\n",
    "        if not nodes:\n",
    "            return None\n",
    "        idxs = sorted(nodes)\n",
    "        T = g[f\"{idxs[0]}.x\"].shape[0]\n",
    "        N = len(idxs)\n",
    "        pts = np.full((T, N, 2), np.nan, dtype=float)\n",
    "        for j, nid in enumerate(idxs):\n",
    "            x = np.array(g[f\"{nid}.x\"], dtype=float)\n",
    "            y = np.array(g[f\"{nid}.y\"], dtype=float)\n",
    "            pts[:, j, 0] = x\n",
    "            pts[:, j, 1] = y\n",
    "            sk = f\"{nid}.score\"\n",
    "            if sk in g:\n",
    "                sc = np.array(g[sk], dtype=float)\n",
    "                bad = ~np.isfinite(sc) | (sc <= 0)\n",
    "                pts[bad, j, :] = np.nan\n",
    "        names = _extract_names(f, N)\n",
    "        fps = _extract_fps(f)\n",
    "        frame_idx = None\n",
    "        if \"frame_idx\" in g:\n",
    "            frame_idx = np.array(g[\"frame_idx\"]).astype(int)\n",
    "        return pts, names, fps, frame_idx\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        got = _try_classic(f)\n",
    "        if got is not None:\n",
    "            return got\n",
    "        got = _try_flat_predictions(f)\n",
    "        if got is not None:\n",
    "            return got\n",
    "        raise RuntimeError(f\"No usable points dataset found in {path}.\")\n",
    "\n",
    "\n",
    "def find_alias_indices(node_names, alias_dict):\n",
    "    names_lc = [n.lower() for n in node_names]\n",
    "    def find_one(cands):\n",
    "        for i, n in enumerate(names_lc):\n",
    "            for c in cands:\n",
    "                if c in n:\n",
    "                    return i\n",
    "        return None\n",
    "    idxU = find_one(alias_dict[\"U\"])\n",
    "    idxL = find_one(alias_dict[\"L\"])\n",
    "    idxH = [i for i, n in enumerate(names_lc) for c in alias_dict[\"HEAD\"] if c in n and i not in (idxU, idxL)][:3]\n",
    "    return idxU, idxL, idxH\n",
    "\n",
    "def name_matches_any(name, substrs):\n",
    "    n = name.lower(); return any(s in n for s in substrs)\n",
    "\n",
    "# ------------------ LOAD CALIBRATION ------------------\n",
    "with open(CALIB_JSON, \"r\") as cf:\n",
    "    calib = json.load(cf)\n",
    "Ps    = {k: np.array(v) for k, v in calib[\"P\"].items()}\n",
    "Ks    = {k: np.array(calib[\"intrinsics\"][k][\"K\"]) for k in Ps}\n",
    "Dists = {k: np.array(calib[\"intrinsics\"][k][\"dist\"]) for k in Ps}\n",
    "IMG_SIZE = tuple(calib.get(\"img_size\", [None, None]))\n",
    "log(f\"[calib] cameras: {list(Ps.keys())}, img_size: {IMG_SIZE}\")\n",
    "\n",
    "# ------------------ LOAD PREDICTIONS ------------------\n",
    "per_cam_pts = {}\n",
    "per_cam_frameidx = {}\n",
    "node_names_ref, fps_guess = None, None\n",
    "for cam in CAM_ORDER:\n",
    "    path = PRED_FILES[cam]\n",
    "    if not path.exists():\n",
    "        log(f\"[warn] missing predictions for {cam}: {path}\")\n",
    "        continue\n",
    "    pts, node_names, fps, fidx = load_sleap_h5(path)\n",
    "    if node_names_ref is None:\n",
    "        node_names_ref = node_names\n",
    "    else:\n",
    "        if node_names != node_names_ref:\n",
    "            log(f\"[warn] node order differs for {cam}; will realign later\")\n",
    "    per_cam_pts[cam] = pts\n",
    "    per_cam_frameidx[cam] = fidx\n",
    "    if fps:\n",
    "        fps_guess = fps\n",
    "    log(f\"[{cam}] loaded {pts.shape} frames, fps≈{fps or 'unknown'} from {path}\")\n",
    "\n",
    "if len(per_cam_pts) < 2:\n",
    "    raise RuntimeError(\"Need ≥2 cameras with tracks to triangulate.\")\n",
    "for cam in per_cam_pts.keys():\n",
    "    if cam not in Ps or cam not in Ks or cam not in Dists:\n",
    "        raise KeyError(f\"Calibration missing for camera key '{cam}'.\")\n",
    "\n",
    "FPS = float(fps_guess or FPS_FALLBACK)\n",
    "log(f\"Using FPS = {FPS}\")\n",
    "\n",
    "# ------------------ TIME ALIGNMENT ------------------\n",
    "def shift_with_nans(arr, shift):\n",
    "    T = arr.shape[0]; out = np.full_like(arr, np.nan)\n",
    "    if shift == 0: return arr.copy()\n",
    "    if shift > 0: out[shift:] = arr[:T-shift]\n",
    "    else: out[:T+shift] = arr[-shift:]\n",
    "    return out\n",
    "\n",
    "def estimate_lag_by_frameidx(fidx_ref, fidx_cam, max_abs_shift=5000):\n",
    "    if fidx_ref is None or fidx_cam is None:\n",
    "        return None\n",
    "    ref_map = {int(v): i for i, v in enumerate(fidx_ref)}\n",
    "    inter = [(ref_map.get(int(v)), i) for i, v in enumerate(fidx_cam) if int(v) in ref_map]\n",
    "    inter = [(ri, ci) for (ri, ci) in inter if ri is not None]\n",
    "    if len(inter) < 100:\n",
    "        return None\n",
    "    diffs = [ri - ci for (ri, ci) in inter]\n",
    "    md = int(np.median(diffs))\n",
    "    if abs(md) > max_abs_shift:\n",
    "        return None\n",
    "    return md\n",
    "\n",
    "lags = {}\n",
    "ref_cam = CAM_ORDER[0]\n",
    "lag_window = int(LAG_WINDOW_SEC * FPS)\n",
    "sample_t_count = min(SAMPLE_FRAMES_ALIGN, per_cam_pts[ref_cam].shape[0])\n",
    "sample_t = np.linspace(0, per_cam_pts[ref_cam].shape[0]-1, num=sample_t_count, dtype=int)\n",
    "\n",
    "Rts = {cam: np.linalg.inv(Ks[cam]) @ Ps[cam] for cam in Ps}\n",
    "\n",
    "def undist_norm(xy, K, dist):\n",
    "    return cv2.undistortPoints(xy.reshape(1,1,2).astype(np.float64), K, dist, P=None).reshape(2)\n",
    "\n",
    "def tri_2views_norm(Rt1, xn1, Rt2, xn2):\n",
    "    A = np.stack([xn1[0]*Rt1[2]-Rt1[0], xn1[1]*Rt1[2]-Rt1[1], xn2[0]*Rt2[2]-Rt2[0], xn2[1]*Rt2[2]-Rt2[1]], axis=0)\n",
    "    _,_,Vt = np.linalg.svd(A); Xh = Vt[-1]; Xh /= Xh[3]; return Xh[:3]\n",
    "\n",
    "def undistort_px(xy, cam):\n",
    "    return cv2.undistortPoints(xy.reshape(1,1,2).astype(np.float64), Ks[cam], Dists[cam], P=Ks[cam]).reshape(2)\n",
    "\n",
    "def pair_err(ref_xy, cam_xy, ref_cam, cam):\n",
    "    xn_ref = undist_norm(ref_xy, Ks[ref_cam], Dists[ref_cam]); xn_cam = undist_norm(cam_xy, Ks[cam], Dists[cam])\n",
    "    X = tri_2views_norm(Rts[ref_cam], xn_ref, Rts[cam], xn_cam)\n",
    "    u_ref = undistort_px(ref_xy, ref_cam); u_cam = undistort_px(cam_xy, cam)\n",
    "    e_ref = np.linalg.norm((Ps[ref_cam] @ np.append(X,1.0))[:2] / (Ps[ref_cam] @ np.append(X,1.0))[2] - u_ref)\n",
    "    e_cam = np.linalg.norm((Ps[cam]      @ np.append(X,1.0))[:2] / (Ps[cam]      @ np.append(X,1.0))[2] - u_cam)\n",
    "    return 0.5*(e_ref+e_cam)\n",
    "\n",
    "lags[ref_cam] = 0\n",
    "for cam in CAM_ORDER[1:]:\n",
    "    if cam not in per_cam_pts:\n",
    "        continue\n",
    "    lag = estimate_lag_by_frameidx(per_cam_frameidx.get(ref_cam), per_cam_frameidx.get(cam))\n",
    "    if lag is not None:\n",
    "        lags[cam] = lag; continue\n",
    "    best_lag, best_med = 0, np.inf\n",
    "    sample_j = list(range(min(SAMPLE_NODES_ALIGN, per_cam_pts[ref_cam].shape[1])))\n",
    "    for lag in range(-lag_window, lag_window+1, max(1,int(FPS//30))):\n",
    "        errs = []\n",
    "        for t in sample_t:\n",
    "            t2 = t - lag\n",
    "            if t2 < 0 or t2 >= per_cam_pts[cam].shape[0]:\n",
    "                continue\n",
    "            for j in sample_j:\n",
    "                xy1 = per_cam_pts[ref_cam][t, j, :]\n",
    "                xy2 = per_cam_pts[cam][t2, j, :]\n",
    "                if np.all(np.isfinite(xy1)) and np.all(np.isfinite(xy2)):\n",
    "                    try:\n",
    "                        errs.append(pair_err(xy1, xy2, ref_cam, cam))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        if len(errs) > 50:\n",
    "            med = float(np.median(errs))\n",
    "            if med < best_med:\n",
    "                best_med, best_lag = med, lag\n",
    "    lags[cam] = best_lag\n",
    "    log(f\"[align] estimated lag {cam} vs {ref_cam}: {best_lag} frames (median reproj err≈{best_med:.2f})\")\n",
    "\n",
    "for cam in CAM_ORDER:\n",
    "    if cam not in per_cam_pts:\n",
    "        continue\n",
    "    per_cam_pts[cam] = shift_with_nans(per_cam_pts[cam], lags.get(cam, 0))\n",
    "\n",
    "lengths = {cam: per_cam_pts[cam].shape[0] for cam in per_cam_pts}\n",
    "T_common = min(lengths.values())\n",
    "per_cam_pts = {cam: per_cam_pts[cam][:T_common].copy() for cam in per_cam_pts}\n",
    "log(f\"[align] applied lags: {lags} → T = {T_common}\")\n",
    "\n",
    "# ------------------ CLEAN 2D ------------------\n",
    "def mark_outliers_speed(pts, max_px_per_frame=MAX_SPEED_PX):\n",
    "    ok = np.ones((pts.shape[0],), dtype=bool)\n",
    "    v = np.linalg.norm(np.diff(pts, axis=0), axis=1)\n",
    "    bad = np.r_[False, v > max_px_per_frame]\n",
    "    bad |= np.any(~np.isfinite(pts), axis=1)\n",
    "    ok[bad] = False\n",
    "    return ok\n",
    "\n",
    "def fill_short_gaps_2d(pts, max_gap=MAX_GAP_2D):\n",
    "    out = pts.copy()\n",
    "    out[:, 0] = interp_short_nan_runs(pts[:, 0], max_gap)\n",
    "    out[:, 1] = interp_short_nan_runs(pts[:, 1], max_gap)\n",
    "    return out\n",
    "\n",
    "for cam in list(per_cam_pts.keys()):\n",
    "    arr = per_cam_pts[cam]\n",
    "    for j in range(arr.shape[1]):\n",
    "        ok = mark_outliers_speed(arr[:, j, :])\n",
    "        arr[~ok, j, :] = np.nan\n",
    "        arr[:, j, :] = fill_short_gaps_2d(arr[:, j, :])\n",
    "    per_cam_pts[cam] = arr\n",
    "log(f\"[clean] per-cam lengths: {lengths} → T = {T_common}\")\n",
    "\n",
    "# ------------------ MATCH NODES ------------------\n",
    "ref_cam = CAM_ORDER[0]\n",
    "T = per_cam_pts[ref_cam].shape[0]\n",
    "sample_frames = np.linspace(0, T-1, num=min(SAMPLE_FRAMES_ALIGN, T), dtype=int)\n",
    "\n",
    "def estimate_node_permutation_to_ref(ref_cam, cam, sample_frames, min_pairs=60, bad_cost=1e6):\n",
    "    Tloc, N = per_cam_pts[ref_cam].shape[0], per_cam_pts[ref_cam].shape[1]\n",
    "    C = np.full((N, N), bad_cost, dtype=float)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            errs = []\n",
    "            for t in sample_frames:\n",
    "                if t < 0 or t >= Tloc or t >= per_cam_pts[cam].shape[0]:\n",
    "                    continue\n",
    "                xy1 = per_cam_pts[ref_cam][t, i, :]\n",
    "                xy2 = per_cam_pts[cam][t, j, :]\n",
    "                if np.all(np.isfinite(xy1)) and np.all(np.isfinite(xy2)):\n",
    "                    try:\n",
    "                        errs.append(pair_err(xy1, xy2, ref_cam, cam))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            if len(errs) >= min_pairs:\n",
    "                C[i, j] = float(np.median(errs))\n",
    "    row_ind, col_ind = linear_sum_assignment(C)\n",
    "    perm = np.array(col_ind, dtype=int)\n",
    "    med_costs = C[row_ind, col_ind]\n",
    "    return perm, med_costs, C\n",
    "\n",
    "for cam in CAM_ORDER:\n",
    "    if cam == ref_cam or cam not in per_cam_pts:\n",
    "        continue\n",
    "    perm, costs, _ = estimate_node_permutation_to_ref(ref_cam, cam, sample_frames)\n",
    "    bad_count = int(np.sum(~np.isfinite(costs)) + np.sum(costs > 80.0))\n",
    "    log(f\"[match] {cam} → {ref_cam}: median costs (first 10) {np.round(costs[:10],1)}\")\n",
    "    if bad_count > len(costs)//2:\n",
    "        log(\"[match][warn] many high costs; check calibration or node consistency.\")\n",
    "    per_cam_pts[cam] = per_cam_pts[cam][:, perm, :]\n",
    "log(\"[match] node ordering aligned across cameras\")\n",
    "\n",
    "# ------------------ OPTIONAL U/L SWAP ------------------\n",
    "rng = default_rng(0)\n",
    "sample_idx = np.arange(0, T, max(1, T // 300))\n",
    "idxU_guess, idxL_guess, _ = find_alias_indices(node_names_ref, ALIAS)\n",
    "swap_votes, total_votes = {c: 0 for c in per_cam_pts}, {c: 0 for c in per_cam_pts}\n",
    "if idxU_guess is not None and idxL_guess is not None:\n",
    "    for cam in per_cam_pts:\n",
    "        if cam == ref_cam:\n",
    "            continue\n",
    "        vs, vt = 0, 0\n",
    "        for t in sample_idx:\n",
    "            refU = per_cam_pts[ref_cam][t, idxU_guess, :]\n",
    "            refL = per_cam_pts[ref_cam][t, idxL_guess, :]\n",
    "            camU = per_cam_pts[cam][t, idxU_guess, :]\n",
    "            camL = per_cam_pts[cam][t, idxL_guess, :]\n",
    "            if not (np.all(np.isfinite(refU)) and np.all(np.isfinite(refL)) and np.all(np.isfinite(camU)) and np.all(np.isfinite(camL))):\n",
    "                continue\n",
    "            e0 = pair_err(refU, camU, ref_cam, cam) + pair_err(refL, camL, ref_cam, cam)\n",
    "            e1 = pair_err(refU, camL, ref_cam, cam) + pair_err(refL, camU, ref_cam, cam)\n",
    "            vt += 1\n",
    "            if e1 < e0:\n",
    "                vs += 1\n",
    "        swap_votes[cam] = vs; total_votes[cam] = vt\n",
    "    cams_to_swap = [c for c in per_cam_pts if c != ref_cam and total_votes[c] > 0 and (swap_votes[c] / total_votes[c]) > 0.65]\n",
    "    log(f\"[swap] votes swap/total: { {c: f'{swap_votes[c]}/{total_votes[c]}' for c in per_cam_pts} }\")\n",
    "    if cams_to_swap:\n",
    "        log(f\"[swap] applying to: {cams_to_swap}\")\n",
    "        for cam in cams_to_swap:\n",
    "            arr = per_cam_pts[cam]\n",
    "            arr[:, [idxU_guess, idxL_guess], :] = arr[:, [idxL_guess, idxU_guess], :]\n",
    "            per_cam_pts[cam] = arr\n",
    "else:\n",
    "    log(\"[swap] U/L aliases not found → skipping swap check\")\n",
    "\n",
    "# ------------------ TRIANGULATION ------------------\n",
    "N = per_cam_pts[ref_cam].shape[1]\n",
    "X3 = np.full((T, N, 3), np.nan, dtype=float)\n",
    "accept_counts = np.zeros(N, dtype=int)\n",
    "\n",
    "def tri_2views(P1, xy1, P2, xy2):\n",
    "    A = np.stack([\n",
    "        xy1[0] * P1[2, :] - P1[0, :],\n",
    "        xy1[1] * P1[2, :] - P1[1, :],\n",
    "        xy2[0] * P2[2, :] - P2[0, :],\n",
    "        xy2[1] * P2[2, :] - P2[1, :],\n",
    "    ], axis=0)\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    X = Vt[-1, :]\n",
    "    X /= X[3]\n",
    "    return X[:3]\n",
    "\n",
    "def reproj_err(P, X, xy):\n",
    "    x = P @ np.append(X, 1.0)\n",
    "    x = x[:2] / x[2]\n",
    "    return float(np.linalg.norm(x - xy))\n",
    "\n",
    "def best_pair_at_t(j, t):\n",
    "    best = None\n",
    "    for c1, c2 in combinations(per_cam_pts.keys(), 2):\n",
    "        xy1 = per_cam_pts[c1][t, j, :]\n",
    "        xy2 = per_cam_pts[c2][t, j, :]\n",
    "        if not (np.all(np.isfinite(xy1)) and np.all(np.isfinite(xy2))):\n",
    "            continue\n",
    "        u1 = undistort_px(xy1, c1)\n",
    "        u2 = undistort_px(xy2, c2)\n",
    "        X  = tri_2views(Ps[c1], u1, Ps[c2], u2)\n",
    "        e1 = reproj_err(Ps[c1], X, u1)\n",
    "        e2 = reproj_err(Ps[c2], X, u2)\n",
    "        e  = 0.5 * (e1 + e2)\n",
    "        if (best is None) or (e < best[0]):\n",
    "            best = (e, (c1, c2), X)\n",
    "    return best\n",
    "\n",
    "for j in range(N):\n",
    "    prev_good = False\n",
    "    for t in range(T):\n",
    "        bp = best_pair_at_t(j, t)\n",
    "        if bp is None:\n",
    "            prev_good = False\n",
    "            continue\n",
    "        e, _, X = bp\n",
    "        if e <= REPROJ_STRICT or (e <= REPROJ_LOOSE and prev_good):\n",
    "            X3[t, j, :] = X\n",
    "            accept_counts[j] += 1\n",
    "            prev_good = True\n",
    "        else:\n",
    "            prev_good = False\n",
    "\n",
    "MAX_GAP_3D = int(MAX_GAP_3D_SEC * FPS)\n",
    "for j in range(N):\n",
    "    for d in range(3):\n",
    "        X3[:, j, d] = interp_short_nan_runs(X3[:, j, d], MAX_GAP_3D)\n",
    "\n",
    "cov_by_node = (np.isfinite(X3).all(axis=2)).mean(axis=0)\n",
    "log(f\"[tri+fill] per-node coverage (first 10): {np.round(cov_by_node[:10], 3)}\")\n",
    "\n",
    "# ------------------ SAVE 3D OUTPUTS ------------------\n",
    "all_npz = OUTDIR / f\"all_nodes_3d_{SUFFIX}.npz\"\n",
    "all_csv = OUTDIR / f\"all_nodes_3d_long_{SUFFIX}.csv\"\n",
    "np.savez_compressed(all_npz, X3=X3.astype(np.float32), node_names=np.array(node_names_ref), FPS=np.float32(FPS))\n",
    "rows = []\n",
    "tvec_full = np.arange(T) / FPS\n",
    "for j, name in enumerate(node_names_ref):\n",
    "    rows.append(pd.DataFrame({\n",
    "        \"frame\": np.arange(T),\n",
    "        \"time_s\": tvec_full,\n",
    "        \"node\": name,\n",
    "        \"x\": X3[:, j, 0], \"y\": X3[:, j, 1], \"z\": X3[:, j, 2],\n",
    "    }))\n",
    "pd.concat(rows, ignore_index=True).to_csv(all_csv, index=False)\n",
    "log(f\"[OK] wrote 3D outputs: {all_npz}, {all_csv}\")\n",
    "\n",
    "# ------------------ GAPE (optional) ------------------\n",
    "def pick_gape_pair(node_names, X3, FPS):\n",
    "    idxU, idxL, _ = find_alias_indices(node_names, ALIAS)\n",
    "    if idxU is not None and idxL is not None:\n",
    "        return idxU, idxL\n",
    "    cand = [i for i,n in enumerate(node_names) if name_matches_any(n, MOUTHY)]\n",
    "    if len(cand) < 2:\n",
    "        cand = list(range(len(node_names)))\n",
    "    cov = np.isfinite(X3).all(axis=2)\n",
    "    best_pair, best_score = (None, None), -np.inf\n",
    "    Tloc = X3.shape[0]\n",
    "    for i, j in combinations(cand, 2):\n",
    "        good = cov[:, i] & cov[:, j]\n",
    "        if np.mean(good) < 0.5:\n",
    "            continue\n",
    "        d = np.full(Tloc, np.nan, dtype=float)\n",
    "        dd = X3[:, i, :] - X3[:, j, :]\n",
    "        g = good.nonzero()[0]\n",
    "        if g.size > 0:\n",
    "            d[g] = np.linalg.norm(dd[g, :], axis=1)\n",
    "        seg = d[np.isfinite(d)]\n",
    "        if seg.size < int(1.0 * FPS):\n",
    "            continue\n",
    "        q10, q90 = np.nanpercentile(seg, [10, 90])\n",
    "        amp = max(0.0, q90 - q10)\n",
    "        covg = np.mean(np.isfinite(d))\n",
    "        score = amp * (0.5 + 0.5 * covg)\n",
    "        if score > best_score:\n",
    "            best_score = score; best_pair = (i, j)\n",
    "    if best_pair[0] is None:\n",
    "        return None, None\n",
    "    i, j = best_pair; zi = np.nanmedian(X3[:, i, 2]); zj = np.nanmedian(X3[:, j, 2])\n",
    "    return (i, j) if zi >= zj else (j, i)\n",
    "\n",
    "idxU, idxL = pick_gape_pair(node_names_ref, X3, FPS)\n",
    "if idxU is not None and idxL is not None:\n",
    "    log(f\"[gape] using nodes: U={node_names_ref[idxU]}  L={node_names_ref[idxL]}  (idx {idxU},{idxL})\")\n",
    "    U3 = X3[:, idxU, :]; L3 = X3[:, idxL, :]\n",
    "    tri_out_csv = OUTDIR / f\"triangulated_UL_3d_{SUFFIX}.csv\"\n",
    "    tri_out_npz = OUTDIR / f\"triangulated_UL_3d_{SUFFIX}.npz\"\n",
    "    pd.DataFrame({\n",
    "        \"frame\": np.arange(T),\n",
    "        \"time_s\": np.arange(T)/FPS,\n",
    "        \"Ux\": U3[:,0], \"Uy\": U3[:,1], \"Uz\": U3[:,2],\n",
    "        \"Lx\": L3[:,0], \"Ly\": L3[:,1], \"Lz\": L3[:,2],\n",
    "    }).to_csv(tri_out_csv, index=False)\n",
    "    np.savez_compressed(tri_out_npz, U3=U3.astype(np.float32), L3=L3.astype(np.float32), FPS=np.float32(FPS))\n",
    "    log(f\"[OK] wrote UL 3D tracks: {tri_out_csv}\")\n",
    "\n",
    "    def odd_leq(n): return max(3, n if n % 2 else n - 1)\n",
    "\n",
    "    gape = np.full(T, np.nan)\n",
    "    good3d = np.all(np.isfinite(U3), 1) & np.all(np.isfinite(L3), 1)\n",
    "    gape[good3d] = np.linalg.norm(U3[good3d] - L3[good3d], axis=1)\n",
    "\n",
    "    def hampel_1d(x, win, k=3.0):\n",
    "        x = x.copy(); n = len(x); h = max(3, win//2)\n",
    "        for i in range(n):\n",
    "            j0, j1 = max(0, i-h), min(n, i+h+1)\n",
    "            s = x[j0:j1]; s = s[np.isfinite(s)]\n",
    "            if s.size < 5 or not np.isfinite(x[i]):\n",
    "                continue\n",
    "            med = np.median(s); mad = np.median(np.abs(s - med)) + 1e-9\n",
    "            if abs(x[i] - med) > k * 1.4826 * mad:\n",
    "                x[i] = np.nan\n",
    "        return x\n",
    "\n",
    "    gape = hampel_1d(gape, int(0.30 * FPS))\n",
    "    gape_mm = gape * 1000.0\n",
    "    dg = np.gradient(gape_mm, 1.0 / FPS)\n",
    "    gape_mm[np.abs(dg) > 350.0] = np.nan\n",
    "    gape = interp_short_nan_runs(gape, int(0.25 * FPS))\n",
    "\n",
    "    PRE_W = odd_leq(int(0.18 * FPS))\n",
    "    BASE_W = odd_leq(int(2.0  * FPS))\n",
    "    g_pre = savgol_filter(np.nan_to_num(gape, nan=np.nanmedian(gape)), PRE_W, 2, mode=\"interp\")\n",
    "    baseline = pd.Series(g_pre).rolling(BASE_W, center=True, min_periods=max(3, BASE_W//3)).min().to_numpy()\n",
    "    gape_zero = np.clip(gape - baseline, 0.0, None)\n",
    "    gape_zero_mm = gape_zero * 1000.0\n",
    "    gape_zero_mm[gape_zero_mm > 50.0] = np.nan\n",
    "    gape_zero_mm = interp_short_nan_runs(gape_zero_mm, int(0.25 * FPS))\n",
    "\n",
    "    SMOOTH_W = odd_leq(int(0.22 * FPS))\n",
    "    gape_s_mm = savgol_filter(np.nan_to_num(gape_zero_mm, nan=np.nanmedian(gape_zero_mm)), SMOOTH_W, 3, mode=\"interp\")\n",
    "    gape_s = gape_s_mm / 1000.0\n",
    "    acc_s  = savgol_filter(gape_s, SMOOTH_W, 3, deriv=2, delta=1.0/FPS, mode=\"interp\")\n",
    "\n",
    "    q = np.nanpercentile(gape_s_mm, [0, 25, 50, 75, 95])\n",
    "    log(f\"[gape] percentiles mm: {q}\")\n",
    "\n",
    "    WSEC = 10.0; win = int(WSEC * FPS); stride = max(1, int(0.05 * FPS))\n",
    "    def robust_amp_mm(xmm):\n",
    "        q10, q90 = np.nanpercentile(xmm, [10, 90]); return q90 - q10\n",
    "\n",
    "    best_f0, best_score = 0, -np.inf\n",
    "    for f0 in range(0, max(1, T - win + 1), stride):\n",
    "        seg = gape_s_mm[f0:f0+win]; cov = np.mean(np.isfinite(seg))\n",
    "        if cov < 0.85: continue\n",
    "        sc = robust_amp_mm(seg) * (0.6 + 0.4*cov)\n",
    "        if sc > best_score:\n",
    "            best_score, best_f0 = sc, f0\n",
    "\n",
    "    f0, f1 = best_f0, min(best_f0 + win, T); tvec = np.arange(f0, f1) / FPS\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 3.2))\n",
    "    ax1.plot(tvec, gape_s_mm[f0:f1], label=\"Gape (smoothed)\")\n",
    "    ax1.set_xlabel(\"Time (s)\"); ax1.set_ylabel(\"Gape (mm)\"); ax1.set_ylim(bottom=0, top=60)\n",
    "    ax2 = ax1.twinx(); ax2.plot(tvec, acc_s[f0:f1]*1000.0, color=\"orange\", label=\"Acceleration\")\n",
    "    ax2.set_ylabel(\"Acceleration (mm/s²)\", color=\"orange\")\n",
    "    h1,l1 = ax1.get_legend_handles_labels(); h2,l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, loc=\"upper right\")\n",
    "    ax1.set_title(\"Vertical gape (zero-baseline) & acceleration — best 10 s\")\n",
    "    plt.tight_layout(); plt.savefig(OUTDIR / f\"gape_10s_overlay_zero_clean_{SUFFIX}.png\"); plt.close()\n",
    "\n",
    "    seg_mm = gape_s_mm[f0:f1]; pks,_ = find_peaks(seg_mm, distance=max(1, int(0.06*FPS)))\n",
    "    trs,_ = find_peaks(-seg_mm, distance=max(1, int(0.06*FPS)))\n",
    "    fig, ax = plt.subplots(figsize=(10, 2.8))\n",
    "    ax.plot(tvec, seg_mm, lw=1.4)\n",
    "    ax.scatter(tvec[pks], seg_mm[pks], s=28, marker=\"o\", label=\"peaks\")\n",
    "    ax.scatter(tvec[trs], seg_mm[trs], s=28, marker=\"v\", label=\"troughs\")\n",
    "    ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Gape (mm)\"); ax.set_ylim(bottom=0, top=60)\n",
    "    ax.set_title(\"Gape — detected cycle landmarks (10 s)\")\n",
    "    ax.legend(); plt.tight_layout(); plt.savefig(OUTDIR / f\"gape_10s_cycles_zero_clean_{SUFFIX}.png\"); plt.close()\n",
    "\n",
    "    if T / FPS > 40.0:\n",
    "        f0L, f1L = int(30*FPS), int(40*FPS); tt = np.arange(f0L, f1L) / FPS\n",
    "        fig, ax1 = plt.subplots(figsize=(16, 3.2))\n",
    "        ax1.plot(tt, gape_s_mm[f0L:f1L], label=\"Gape (smoothed)\"); ax1.set_xlabel(\"time (s)\"); ax1.set_ylabel(\"gape (mm)\"); ax1.set_ylim(bottom=0, top=60)\n",
    "        ax2 = ax1.twinx(); ax2.plot(tt, acc_s[f0L:f1L]*1000.0, color=\"orange\", label=\"Acceleration\"); ax2.set_ylabel(\"acceleration (mm/s²)\", color=\"orange\")\n",
    "        h1,l1 = ax1.get_legend_handles_labels(); h2,l2 = ax2.get_legend_handles_labels(); ax1.legend(h1+h2, l1+l2, loc=\"upper right\")\n",
    "        ax1.set_title(\"Vertical gape displacement & acceleration — 30–40 s\")\n",
    "        plt.tight_layout(); plt.savefig(OUTDIR / f\"gape_30to40s_overlay_zero_clean_{SUFFIX}.png\"); plt.close()\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"frame\": np.arange(T),\n",
    "        \"time_s\": np.arange(T)/FPS,\n",
    "        \"gape_m_raw\": gape,\n",
    "        \"gape_m_zero\": gape_s_mm/1000.0,\n",
    "        \"gape_mm_zero\": gape_s_mm,\n",
    "        \"acc_m_per_s2\": acc_s,\n",
    "    }).to_csv(OUTDIR / f\"gape_timeseries_from_sleap_zero_{SUFFIX}.csv\", index=False)\n",
    "    log(\"[DONE] Wrote gape time-series and plots.\")\n",
    "else:\n",
    "    log(\"[info] Could not identify a reliable U/L pair → skipped gape. 3D for all nodes was saved.\")\n",
    "\n",
    "log(\"Phase 0 triangulation complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc3ab1-07c6-406e-9a91-17caa1279a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sleap-3712)",
   "language": "python",
   "name": "sleap-3712"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
